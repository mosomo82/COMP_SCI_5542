# CS 5542 â€” Week 5 Snowflake Integration Starter

This starter kit provides a minimal, reproducible **Data â†’ Snowflake â†’ Query â†’ App â†’ Logging** pipeline.

## Repo Layout
- `sql/`: schema, staging/loading, and query examples
- `scripts/`: connection + local CSV â†’ stage â†’ COPY loader
- `app/`: Streamlit dashboard connected to Snowflake
- `data/`: sample CSVs (replace with your project subset)
- `logs/`: pipeline usage logs
- `CONTRIBUTIONS.md`: individual accountability

## Week 5 Scope (â‰ˆ50%)

| Item | Included this week | Deferred |
|---|---|---|
| Dataset(s) | All 14 trucking tables (customers, drivers, trucks, routes, loads, trips, fuel_purchases + trailers, facilities, delivery_events, maintenance_records, safety_incidents, driver/truck metrics) | â€” |
| Feature(s) | Schema + staging + COPY INTO, 5 analytical queries, 5 views, batch Python loader, 5-tab Streamlit dashboard, pipeline monitoring | â€” |

## End-to-End Flow
```mermaid
flowchart LR
A[14 CSVs â€” Trucking Data] --> B[Snowflake Stage + COPY]
B --> C["7 Tables (4 dim + 3 fact)"]
C --> D[5 Views]
D --> E[4-Tab Streamlit Dashboard]
E --> F[Monitoring Logs]
```

## Setup
1) Create `.env` from `.env.example` and fill your Snowflake values.
2) Install dependencies:
```bash
pip install -r requirements.txt
```

## Snowflake SQL Setup
Run these scripts in a Snowflake Worksheet (in order):
1. `sql/01_create_schema.sql` â€” creates database + 7 tables
2. `sql/02_stage_and_load.sql` â€” warehouse, file format, stage, COPY INTO
3. `sql/04_views.sql` â€” 5 derived views for the dashboard

## Load Data

### Batch (all 7 tables at once â€” recommended)
```bash
python scripts/load_local_csv_to_stage.py --batch
```

### Single table
```bash
python scripts/load_local_csv_to_stage.py data/customers.csv CUSTOMERS
python scripts/load_local_csv_to_stage.py data/drivers.csv DRIVERS
python scripts/load_local_csv_to_stage.py data/trucks.csv TRUCKS
python scripts/load_local_csv_to_stage.py data/routes.csv ROUTES
python scripts/load_local_csv_to_stage.py data/loads.csv LOADS
python scripts/load_local_csv_to_stage.py data/trips.csv TRIPS
python scripts/load_local_csv_to_stage.py data/fuel_purchases.csv FUEL_PURCHASES
```

## Analytical Queries
Run `sql/03_queries.sql` after loading data:
1. **Q1: Revenue by customer** â€” top customers by total completed-load revenue
2. **Q2: Driver fuel efficiency** â€” avg MPG per driver, ranked
3. **Q3: Route profitability** â€” revenue minus fuel cost per route (4-table join)
4. **Q4: Monthly revenue trend** â€” time-series analysis with DATE_TRUNC
5. **Q5: Truck fleet utilization** â€” filtered multi-join with aggregation

## Dashboard
```bash
streamlit run app/streamlit_app.py
```

| Tab | Description |
|---|---|
| ğŸ“Š Overview | KPI cards + monthly revenue line chart (date-range filter) |
| ğŸš› Fleet & Drivers | Truck/driver performance (fuel-type multi-select, min-trips slider) |
| ğŸ—ºï¸ Routes | Route scorecard (margin threshold, min-loads filter) |
| â›½ Fuel Spend | Fuel spend by state (state filter, top-N slider) |

## Extensions Completed
- **Extension 1: Full dataset ingestion** â€” ingested all 14 trucking CSVs (added trailers, facilities, delivery_events, maintenance_records, safety_incidents, driver_monthly_metrics, truck_utilization_metrics)
- **Extension 2: Pipeline monitoring** â€” auto-logging with `perf_note`, latency charts, per-query stats, and performance summary in `ğŸ“ˆ Monitoring` tab

## Demo Video Link
- 

## Notes / Bottlenecks
- 
