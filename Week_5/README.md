# ðŸšš CS 5542 â€” Week 5: Logistics Operation Dashboard

*A minimal, reproducible Data Engineering pipeline: Data â†’ Snowflake â†’ Query â†’ App â†’ Logging*

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg?style=for-the-badge&logo=python&logoColor=white)](#)
[![Snowflake](https://img.shields.io/badge/Snowflake-Data_Warehouse-29B5E8.svg?style=for-the-badge&logo=snowflake&logoColor=white)](#)
[![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-FF4B4B.svg?style=for-the-badge&logo=streamlit&logoColor=white)](#)

</div>

---

## ðŸ”— Live Deployment

> **Streamlit Cloud:** [ðŸš€ Launch Lab5_Streamlit_Cloud](https://cs5542lab5.streamlit.app/)

---

## ðŸ—ï¸ Pipeline Architecture & End-to-End Flow

![logistics_pipeline_architecture_1771661265221](https://github.com/user-attachments/assets/972aed5f-27d4-442e-bce0-eb888fb42cd1)


---

## ðŸ—‚ï¸ Project Structure

Below is the complete directory structure for the Week 5 Logistics Dashboard pipeline. It is organized into distinct layers for data generation, SQL execution, Python orchestration, and front-end visualization.

```text
Week_5/
â”œâ”€â”€ app/                        # Presentation Layer
â”‚   â””â”€â”€ streamlit_app.py        # Main 7-tab Streamlit dashboard application
â”œâ”€â”€ data/                       # Synthetic Logistics Datasets (14 Core/Ext Tables)
â”‚   â”œâ”€â”€ customers.csv           # 200 rows 
â”‚   â”œâ”€â”€ drivers.csv             # 150 rows
â”‚   â”œâ”€â”€ trucks.csv              # 100 rows
â”‚   â”œâ”€â”€ routes.csv              # 80 rows
â”‚   â”œâ”€â”€ loads.csv               # 500 rows
â”‚   â”œâ”€â”€ trips.csv               # 600 rows
â”‚   â”œâ”€â”€ fuel_purchases.csv      # 400 rows
â”‚   â”œâ”€â”€ trailers.csv            # Extension table
â”‚   â”œâ”€â”€ facilities.csv          # Extension table
â”‚   â”œâ”€â”€ delivery_events.csv     # Extension table
â”‚   â”œâ”€â”€ maintenance_records.csv # Extension table
â”‚   â”œâ”€â”€ safety_incidents.csv    # Extension table
â”‚   â”œâ”€â”€ driver_monthly_metrics.csv
â”‚   â””â”€â”€ truck_utilization_metrics.csv
â”œâ”€â”€ logs/                       # Pipeline Monitoring
â”‚   â””â”€â”€ pipeline_logs.csv       # Tracks ingestion success, errors, and latencies
â”œâ”€â”€ scripts/                    # Ingestion & Orchestration Scripts
â”‚   â”œâ”€â”€ load_local_csv_to_stage.py # Local/Batch ingestion using internal staging
â”‚   â””â”€â”€ run_pipeline.py         # Automated master orchestrator (Local & S3 modes)
â”œâ”€â”€ sql/                        # Data Warehouse Definition & Analytics
â”‚   â”œâ”€â”€ 01_create_schema.sql    # DDL for Database, Schema, and 14 Tables
â”‚   â”œâ”€â”€ 02_stage_and_load.sql   # Warehouse config, Internal Stage, COPY INTO
â”‚   â”œâ”€â”€ 03_queries.sql          # 5 core analytical queries (Revenue, Efficiency, etc.)
â”‚   â”œâ”€â”€ 04_views.sql            # 5 derived views for dashboard visualization
â”‚   â”œâ”€â”€ 05_derived_analytics.sql# 4 advanced materialized analytics tables
â”‚   â””â”€â”€ 06_s3_pipeline.sql      # AWS Storage Integration & External Stage setup
â”œâ”€â”€ .env.example                # Template for Snowflake & AWS credentials
â”œâ”€â”€ CONTRIBUTIONS.md            # Individual team member accountability log
â”œâ”€â”€ README.md                   # Primary documentation
â””â”€â”€ requirements.txt            # Python package dependencies
```
---

## ðŸŽ¯ Week 5 Scope (â‰ˆ50%)

| Scope Item | Included this week | Deferred |
| --- | --- | --- |
| **Dataset(s)** | All 14 trucking tables *(customers, drivers, trucks, routes, loads, trips, fuel_purchases + trailers, facilities, delivery_events, maintenance_records, safety_incidents, driver/truck metrics)* | â€” |
| **Feature(s)** | Schema + staging + `COPY INTO`, 5 analytical queries, 5 views, batch Python loader, 5-tab Streamlit dashboard, pipeline monitoring | â€” |

---

## ðŸš€ Setup Instructions

**1. Configure Environment**
Copy the example environment file and fill in your Snowflake credentials.

```bash
cp .env.example .env

```

**2. Install Dependencies**

```bash
pip install -r requirements.txt

```

---

## â„ï¸ Snowflake SQL Setup

Run these scripts in a Snowflake Worksheet (in order).

> ðŸ’¡ **Tip:** Steps 1â€“5 are fully automated by `scripts/run_pipeline.py` (see the Load Data section below).

1. `sql/01_create_schema.sql` â€” Creates database + 14 tables.
2. `sql/02_stage_and_load.sql` â€” Configures warehouse, file format, internal stage, and executes `COPY INTO`.
3. `sql/04_views.sql` â€” Generates 5 derived views for the dashboard.
4. `sql/05_derived_analytics.sql` â€” Builds 4 advanced derived analytics tables.
5. `sql/06_s3_pipeline.sql` â€” Sets up S3 storage integration, external stage, and runs `COPY INTO` from S3.

---

## ðŸ“¥ Load Data

Choose your preferred ingestion method below:

### ðŸ¤– Automated Pipeline (Recommended)

Load all 14 tables from S3, build views + derived analytics, and log the run in one command:

```bash
python scripts/run_pipeline.py

```

*Alternative pipeline flags:*

```bash
# Use local CSVs instead of S3
python scripts/run_pipeline.py --local

# Skip storage integration creation (if it already exists)
python scripts/run_pipeline.py --skip-s3-setup

```

### ðŸ“‚ Manual Batch (Internal Stage)

```bash
python scripts/load_local_csv_to_stage.py --batch

```

### ðŸ“„ Single Table Loading

```bash
python scripts/load_local_csv_to_stage.py data/customers.csv CUSTOMERS
python scripts/load_local_csv_to_stage.py data/drivers.csv DRIVERS
# ... repeat for other tables as needed

```

---

## ðŸ§  Analytical Queries

Run `sql/03_queries.sql` after loading your data to test these core insights:

1. **Revenue by Customer:** Top customers by total completed-load revenue.
2. **Driver Fuel Efficiency:** Average MPG per driver, ranked.
3. **Route Profitability:** Revenue minus fuel cost per route (4-table join).
4. **Monthly Revenue Trend:** Time-series analysis utilizing `DATE_TRUNC`.
5. **Truck Fleet Utilization:** Filtered multi-join with aggregation.

---

## ðŸ“ˆ Streamlit Dashboard

Launch the interactive application locally:

```bash
streamlit run app/streamlit_app.py

```

### Dashboard Layout

| Tab | Features |
| --- | --- |
| ðŸ“Š **Overview** | KPI cards + monthly revenue line chart (date-range filter). |
| ðŸš› **Fleet & Drivers** | Truck/driver performance (fuel-type multi-select, min-trips slider). |
| ðŸ—ºï¸ **Routes** | Route scorecard (margin threshold, min-loads filter). |
| â›½ **Fuel Spend** | Fuel spend by state (state filter, top-N slider). |
| ðŸ“ˆ **Monitoring** | Performance stats, latency over time, and raw query logs. |
| ðŸ”¬ **Analytics** | Advanced derived tables (Driver rankings, Truck health, Route quality). |
| ðŸŽ¯ **Executive** | Auto-loading KPIs, terminal heatmap, and a live SQL explorer. |

---

## âœ¨ Extensions Completed

* [x] **Extension 1: Full dataset ingestion** â€” Ingested all 14 trucking CSVs, including trailers, facilities, maintenance_records, and more.
* [x] **Extension 2: Pipeline monitoring** â€” Dedicated `ðŸ“ˆ Monitoring` tab with performance summary, latency charts, and query stats.
* [x] **Extension 3: Advanced derived analytics** â€” `05_derived_analytics.sql` creates materialized tables for driver rankings, truck health, and route quality.
* [x] **Extension 4: Automated S3 ingestion pipeline** â€” `scripts/run_pipeline.py` provides one-command orchestration for schema creation and S3 data loading.
* [x] **Extension 5: Interactive executive dashboard** â€” `ðŸŽ¯ Executive` tab with auto-loading KPIs, heatmap, and a live SQL explorer.

---

## ðŸ“‚ Data Source

All 14 CSV datasets in `data/` are sourced from the publicly available Kaggle dataset:

> **[Logistics Operations Database](https://www.kaggle.com/datasets/yogape/logistics-operations-database)**  
> Kaggle Â· *yogape* Â· Logistics / Transportation domain

The CSVs were downloaded as-is and ingested into Snowflake via the staging scripts in `scripts/`.

---

## ðŸŽ¬ Demo & Notes

* **Demo Video Link:** *(Insert link here)*
  
* **Notes / Bottlenecks:**
  
    - **Security & IAM**: ACCOUNTADMIN is required to run the `STORAGE INTEGRATION` step once. AWS IAM roles must manually trust the Snowflake principal (found via `DESCRIBE`) for the S3 pipeline to work.
    - **Data Freshness**: The derived tables in `05_derived_analytics.sql` are "on-demand" materialized. They need a manual re-run (or a Snowflake Task) after new data loads to reflect the latest rankings and scores.
    - **Scaling Limits**: The Streamlit SQL Explorer caps results at 500 rows to prevent browser OOM. For massive datasets (>1M rows), analytics should be pushed to Snowflake views rather than processed in Pandas.
    - **Cold Starts**: The first query in a session may take 5â€“10s if the Snowflake warehouse is suspended. Subsequent cached queries (TTL: 2 min) are sub-second.
    - **S3 Connectivity**: The pipeline assumes CSVs are in the `/data/` prefix of the bucket. Metadata mismatches in S3 will cause `COPY INTO` failures logged in `pipeline_logs.csv`.
 
---

## ðŸ“„ License

This project is for academic use as part of CS 5542 at the University of Missouriâ€“Kansas City.
