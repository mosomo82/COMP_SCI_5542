{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3e229a",
   "metadata": {
    "id": "df3e229a"
   },
   "source": [
    "# CS 5542 — Lab 4 Notebook (Team Project)\n",
    "## RAG Application Integration, Deployment, and Monitoring (Deadline: Feb. 12, 2026)\n",
    "\n",
    "**Purpose:** This notebook is a **project-aligned template** for Lab 4. Your team should reuse your Lab-3 multimodal RAG pipeline and integrate it into a **deployable application** with **automatic logging** and **failure analysis**.\n",
    "\n",
    "### Submission policy\n",
    "- **Survey:** submitted **individually**\n",
    "- **Deliverables (GitHub repo / notebook / report / deployment link):** submitted **as a team**\n",
    "\n",
    "### Team-size requirement\n",
    "- **1–2 students:** Base requirements + **1 extension**\n",
    "- **3–4 students:** Base requirements + **2–3 extensions**\n",
    "\n",
    "---\n",
    "\n",
    "## What you will build (at minimum)\n",
    "1. A **Streamlit app** that accepts a question and returns:\n",
    "   - an **answer**\n",
    "   - **retrieved evidence** with citations\n",
    "   - **metrics panel** (latency, P@5, R@10 if applicable)\n",
    "2. An **automatic logger** that appends to: `logs/query_metrics.csv`\n",
    "3. A **mini gold set** of **5 project queries** (Q1–Q5) for evaluation\n",
    "4. **Two failure cases** with root cause + proposed fix\n",
    "\n",
    "> **Important:** Lab 4 focuses on **application integration and deployment**, not on redesigning retrieval. Prefer reusing your Lab-3 modules.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommended repository structure (for your team repo)\n",
    "```\n",
    "/app/              # Streamlit UI (required)\n",
    "/rag/              # Retrieval + indexing modules (reuse from Lab 3)\n",
    "/logs/             # query_metrics.csv (auto-created)\n",
    "/data/             # your project-aligned PDFs/images (do NOT commit large/private data)\n",
    "/api/              # optional FastAPI backend (extension)\n",
    "/notebooks/        # this notebook\n",
    "requirements.txt\n",
    "README.md\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Contents of this notebook\n",
    "1. Setup & environment checks  \n",
    "2. Project dataset wiring (connect your Lab-3 ingestion)  \n",
    "3. Mini gold set (Q1–Q5)  \n",
    "4. Retrieval + answer function (reuse your Lab-3 pipeline)  \n",
    "5. Evaluation + logging (required)  \n",
    "6. Streamlit app skeleton (required)  \n",
    "7. Optional extension: FastAPI backend  \n",
    "8. Deployment checklist + failure analysis template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nC-fL9Z4Q-8g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nC-fL9Z4Q-8g",
    "outputId": "149c2076-85d8-4d5f-cbd1-75226e338313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted demo files from: data.zip\n",
      "Docs folder exists: True\n",
      "Images folder exists: True\n",
      "Sample docs: ['07_numeric_table.txt', '1041_Enhancing_Time_Series_For.pdf', '4043_ReMindRAG_Low_Cost_LLM_Gu.pdf', 'consensus-planning-with-primal-dual-and-proximal-agents.pdf']\n",
      "Sample images: ['1_image-of-supply-chain-network-optimization.png', 'Developments-of-ADMM.png', 'figure1.png', 'figure10.png', 'figure11.png', 'figure12.png']\n"
     ]
    }
   ],
   "source": [
    "# Task: Load provided demo files (docs + images)\n",
    "# Place Your_zip_files.zip next to this notebook or upload it to the runtime.\n",
    "import os, zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "ZIP_NAME = 'data.zip'\n",
    "zip_candidates = [\n",
    "    ZIP_NAME,\n",
    "    f'/mnt/data/{ZIP_NAME}',\n",
    "    f'./{ZIP_NAME}',\n",
    "]\n",
    "\n",
    "zip_path = None\n",
    "for p in zip_candidates:\n",
    "    if os.path.exists(p):\n",
    "        zip_path = p\n",
    "        break\n",
    "\n",
    "if zip_path is None:\n",
    "    print('⚠️ Demo ZIP not found. If running in Colab/Kaggle, upload:', ZIP_NAME)\n",
    "else:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall('.')\n",
    "    print('✅ Extracted demo files from:', zip_path)\n",
    "\n",
    "# Expected folder structure after extraction:\n",
    "# ./data/docs/*.txt\n",
    "# ./data/images/*.(png|jpg)\n",
    "print('Docs folder exists:', os.path.isdir('./data/docs'))\n",
    "print('Images folder exists:', os.path.isdir('./data/images'))\n",
    "\n",
    "if os.path.isdir('./data/docs'):\n",
    "    print('Sample docs:', sorted(os.listdir('./data/docs'))[:6])\n",
    "if os.path.isdir('./data/images'):\n",
    "    print('Sample images:', sorted(os.listdir('./data/images'))[:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Jdz6kAJaThoY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jdz6kAJaThoY",
    "outputId": "ecc0dc7c-75d5-475d-de79-5282dca4e80c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Numeric demo file already present: ./data/docs/07_numeric_table.txt\n"
     ]
    }
   ],
   "source": [
    "# Ensure a numeric/table-like demo file exists for Q4\n",
    "import os\n",
    "numeric_path = './data/docs/07_numeric_table.txt'\n",
    "if not os.path.exists(numeric_path):\n",
    "    with open(numeric_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\n",
    "            'Fusion Hyperparameters (Table 1)\\n'\n",
    "            'alpha = 0.50\\n'\n",
    "            'top_k = 5\\n'\n",
    "            'missing_evidence_score_threshold = 0.05\\n'\n",
    "            'latency_alert_ms = 2000\\n'\n",
    "        )\n",
    "    print('✅ Created:', numeric_path)\n",
    "else:\n",
    "    print('✅ Numeric demo file already present:', numeric_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8AOvgEV9Q-8g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8AOvgEV9Q-8g",
    "outputId": "62227550-8d5d-4997-9e25-4fb0e20880a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .txt docs: 1\n",
      "Preview: 07_numeric_table.txt\n",
      "Fusion Hyperparameters (Table 1)\n",
      "alpha = 0.50\n",
      "top_k = 5\n",
      "missing_evidence_score_threshold = 0.05\n",
      "latency_alert_ms = 2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks: ensure demo docs are loaded\n",
    "import os, glob\n",
    "doc_files = glob.glob('./data/docs/*.txt')\n",
    "print('Found .txt docs:', len(doc_files))\n",
    "assert len(doc_files) > 0, 'No docs found. Ensure the demo ZIP was extracted and ./data/docs exists.'\n",
    "\n",
    "# Preview one document\n",
    "with open(doc_files[0], 'r', encoding='utf-8') as f:\n",
    "    preview = f.read()[:600]\n",
    "print('Preview:', os.path.basename(doc_files[0]))\n",
    "print(preview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "xGbb-SjVSN21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGbb-SjVSN21",
    "outputId": "1cc684be-e79b-4a58-cc89-75609ecc8385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded documents: 1\n",
      "Example doc_id: 07_numeric_table.txt\n",
      "Fusion Hyperparameters (Table 1)\n",
      "alpha = 0.50\n",
      "top_k = 5\n",
      "missing_evidence_score_threshold = 0.05\n",
      "latency_alert_ms = 2000\n"
     ]
    }
   ],
   "source": [
    "# Load demo documents into a single list used throughout the notebook\n",
    "import glob, os\n",
    "\n",
    "DOC_DIR = './data/docs'\n",
    "doc_files = sorted(glob.glob(os.path.join(DOC_DIR, '*.txt')))\n",
    "if len(doc_files) == 0:\n",
    "    raise RuntimeError('No .txt documents found in ./data/docs. Ensure the demo ZIP was extracted.')\n",
    "\n",
    "documents = []\n",
    "for p in doc_files:\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        txt = f.read().strip()\n",
    "    if not txt:\n",
    "        continue\n",
    "    doc_id = os.path.basename(p)\n",
    "    documents.append({'doc_id': doc_id, 'source': p, 'text': txt})\n",
    "\n",
    "print('✅ Loaded documents:', len(documents))\n",
    "print('Example doc_id:', documents[0]['doc_id'])\n",
    "print(documents[0]['text'][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vLkoKvfRbK41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLkoKvfRbK41",
    "outputId": "54ea2271-99f4-40ce-9e7e-2ffd2e4e0f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded images: 15\n",
      "Example image: 1_image-of-supply-chain-network-optimization.png\n",
      "Caption: 1 image-of-supply-chain-network-optimization\n",
      "✅ Unified evidence items: 16 (text: 1 , images: 15 )\n"
     ]
    }
   ],
   "source": [
    "# Load demo images and create lightweight text surrogates (captions) for multimodal retrieval\n",
    "import glob, os\n",
    "\n",
    "IMG_DIR = './data/images'\n",
    "img_files = sorted(glob.glob(os.path.join(IMG_DIR, '*.*')))\n",
    "img_files = [p for p in img_files if p.lower().endswith(('.png','.jpg','.jpeg','.webp'))]\n",
    "\n",
    "# Minimal captions so images participate in retrieval without requiring a vision encoder\n",
    "IMAGE_CAPTIONS = {\n",
    "    'rag_pipeline.png': 'RAG pipeline diagram: ingest, chunk, index, retrieve top-k evidence, build context, generate grounded answer, log metrics for monitoring.',\n",
    "    'retrieval_modes.png': 'Retrieval modes diagram: BM25 keyword, vector semantic, hybrid fusion, multi-hop hop-1 to hop-2 refinement.',\n",
    "}\n",
    "\n",
    "images = []\n",
    "for p in img_files:\n",
    "    fid = os.path.basename(p)\n",
    "    cap = IMAGE_CAPTIONS.get(fid, fid.replace('_',' ').replace('.png','').replace('.jpg',''))\n",
    "    images.append({'img_id': fid, 'source': p, 'text': cap})\n",
    "\n",
    "print('✅ Loaded images:', len(images))\n",
    "if images:\n",
    "    print('Example image:', images[0]['img_id'])\n",
    "    print('Caption:', images[0]['text'])\n",
    "\n",
    "# Unified evidence store used by retrieval (text + images)\n",
    "items = []\n",
    "for d in documents:\n",
    "    items.append({\n",
    "        'evidence_id': d.get('doc_id') or os.path.basename(d.get('source','')),\n",
    "        'modality': 'text',\n",
    "        'source': d.get('source'),\n",
    "        'text': d.get('text','')\n",
    "    })\n",
    "for im in images:\n",
    "    items.append({\n",
    "        'evidence_id': f\"img::{im['img_id']}\",\n",
    "        'modality': 'image',\n",
    "        'source': im.get('source'),\n",
    "        'text': im.get('text','')\n",
    "    })\n",
    "\n",
    "assert len(items) > 0, 'Evidence store is empty.'\n",
    "print('✅ Unified evidence items:', len(items), '(text:', len(documents), ', images:', len(images), ')')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e549d1",
   "metadata": {
    "id": "47e549d1"
   },
   "source": [
    "# 1) Setup & environment checks\n",
    "\n",
    "This notebook includes **safe defaults** and **lightweight code examples**.  \n",
    "Replace the placeholder pieces with your Lab-3 implementation (PDF parsing, OCR, multimodal evidence, hybrid retrieval, reranking).\n",
    "\n",
    "### Install dependencies (edit as needed)\n",
    "- Core: `streamlit`, `pandas`, `numpy`, `requests`\n",
    "- Optional: `fastapi`, `uvicorn` (if you do the FastAPI extension)\n",
    "- Retrieval examples: `scikit-learn` (TF-IDF baseline), optionally `sentence-transformers` (dense embeddings)\n",
    "\n",
    "> In your team repo, always keep a clean `requirements.txt` for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "425991a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "425991a8",
    "outputId": "120fa141-b899-4463-d1a0-81c1ca1988d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hPython OK. Working directory: /content\n"
     ]
    }
   ],
   "source": [
    "# If running in Colab or fresh environment, uncomment installs:\n",
    "!pip -q install streamlit pandas numpy requests scikit-learn\n",
    "# Optional (FastAPI extension):\n",
    "!pip -q install fastapi uvicorn pydantic\n",
    "# Optional (dense retrieval):\n",
    "!pip -q install sentence-transformers\n",
    "\n",
    "import os, json, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Python OK. Working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c805bf7",
   "metadata": {
    "id": "5c805bf7"
   },
   "source": [
    "# 2) Project paths + configuration\n",
    "\n",
    "Set your project data paths and key parameters here.\n",
    "\n",
    "- Do **not** hardcode secrets (API keys) in notebooks or repos.\n",
    "- If you use a hosted LLM, read from environment variables locally.\n",
    "\n",
    "**Tip:** Keep these settings mirrored in `rag/config.py` so your Streamlit app uses the same config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d483405",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d483405",
    "outputId": "eca019fe-9c5e-4145-dc75-ef1a42b63c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab4Config(project_name='YOUR_PROJECT_NAME', data_dir='./data', logs_dir='./logs', log_file='./logs/query_metrics.csv', top_k_default=10, eval_p_at=5, eval_r_at=10)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Lab4Config:\n",
    "    project_name: str = \"YOUR_PROJECT_NAME\"\n",
    "    data_dir: str = \"./data\"        # where your PDFs/images live locally\n",
    "    logs_dir: str = \"./logs\"\n",
    "    log_file: str = \"./logs/query_metrics.csv\"\n",
    "    top_k_default: int = 10\n",
    "    eval_p_at: int = 5\n",
    "    eval_r_at: int = 10\n",
    "\n",
    "cfg = Lab4Config()\n",
    "Path(cfg.logs_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5030e",
   "metadata": {
    "id": "d5c5030e"
   },
   "source": [
    "# 3) Dataset wiring (project-aligned)\n",
    "\n",
    "For Lab 4, your **data, application UI, and models** must be aligned to your team project.\n",
    "\n",
    "## Required (project-aligned)\n",
    "- 2–6 PDFs\n",
    "- 5–15 images/figures/tables (if your project is multimodal)\n",
    "\n",
    "## In Lab 3 you likely had:\n",
    "- PDF text extraction (PyMuPDF)\n",
    "- OCR / captions for figures or scanned pages\n",
    "- Chunking + indexing (dense/sparse/hybrid)\n",
    "- Reranking (optional)\n",
    "- Grounded answer generation with citations\n",
    "\n",
    "### What to do here\n",
    "1. Point this notebook to your dataset folder.\n",
    "2. Load *already-prepared* chunks/evidence from Lab 3 (recommended), OR\n",
    "3. Call your Lab-3 ingestion function to rebuild the index.\n",
    "\n",
    "Below is a **minimal example** that loads plain text files as “documents” so the notebook is runnable even without PDFs.\n",
    "Replace it with your Lab-3 ingestion code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20f0f725",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20f0f725",
    "outputId": "78118ada-9eba-4640-f705-826867e35636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded docs: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['doc_id', 'source', 'text']), '07_numeric_table')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimal runnable loader (replace with your Lab-3 ingestion + chunking)\n",
    "# Expected structure (example):\n",
    "# ./data/\n",
    "#   docs/\n",
    "#     doc1.txt\n",
    "#     doc2.txt\n",
    "#\n",
    "# For PDFs/images, reuse your Lab-3 ingestion + chunking and store chunks as JSONL/CSV.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "docs_dir = Path(cfg.data_dir) / \"docs\"\n",
    "docs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a tiny demo corpus if empty (so the notebook runs)\n",
    "demo_file = docs_dir / \"demo_doc.txt\"\n",
    "if not any(docs_dir.glob(\"*.txt\")):\n",
    "    demo_file.write_text(\n",
    "        \"This is a demo document for Lab 4. Replace this with your project PDFs and evidence chunks.\\n\"\n",
    "        \"Key idea: retrieval quality drives grounded answers. Provide citations for all claims.\\n\"\n",
    "        \"If missing evidence, return: Not enough evidence in the retrieved context.\\n\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "def load_text_docs(docs_path: Path):\n",
    "    items = []\n",
    "    for p in sorted(docs_path.glob(\"*.txt\")):\n",
    "        items.append({\"doc_id\": p.stem, \"source\": str(p), \"text\": p.read_text(errors='ignore')})\n",
    "    return items\n",
    "\n",
    "documents = load_text_docs(docs_dir)\n",
    "print(\"Loaded docs:\", len(documents))\n",
    "documents[0].keys(), documents[0][\"doc_id\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DHXt-e0mRCiU",
   "metadata": {
    "id": "DHXt-e0mRCiU"
   },
   "source": [
    "# DATA INGESTION & CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "gSA7VXGvR3E3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSA7VXGvR3E3",
    "outputId": "9ecca8fd-d795-4c98-e1c4-d7e9ab6cb157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.7\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, re, glob, json, math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install PyMuPDF\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bgTRlS3aS4GF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgTRlS3aS4GF",
    "outputId": "2b183f5c-a2e8-4ade-b8e6-3cc864035ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs: 3 ['data/docs/doc1_TimeSeries.pdf', 'data/docs/doc2_ReMindRAG.pdf', 'data/docs/doc3_CPP.pdf']\n",
      "Images: 15 ['data/images/figure1.png', 'data/images/figure10.png', 'data/images/figure11.png', 'data/images/figure12.png', 'data/images/figure13.png', 'data/images/figure14.png', 'data/images/figure15.png', 'data/images/figure2.png', 'data/images/figure3.png', 'data/images/figure4.png', 'data/images/figure5.png', 'data/images/figure6.png', 'data/images/figure7.png', 'data/images/figure8.png', 'data/images/figure9.png']\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"data\"\n",
    "DOC_DIR = os.path.join(DATA_DIR, \"docs\")\n",
    "FIG_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "os.makedirs(DOC_DIR, exist_ok=True)\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "pdfs = sorted(glob.glob(os.path.join(DOC_DIR, \"*.pdf\")))\n",
    "imgs = sorted(glob.glob(os.path.join(FIG_DIR, \"*.*\")))\n",
    "\n",
    "print(\"PDFs:\", len(pdfs), pdfs)\n",
    "print(\"Images:\", len(imgs), imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "DS9ETw6_fs79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DS9ETw6_fs79",
    "outputId": "8ce85085-9395-4642-df6c-072ddae84bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -y tesseract-ocr\n",
    "!pip install -q pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "VgSZE7FEflPn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgSZE7FEflPn",
    "outputId": "403d9097-c2c7-4d81-e8af-d4446e471c93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning images in data/images with OCR...\n",
      "Total text chunks: 98\n",
      "Total images: 15\n",
      "Sample text chunk: doc1_TimeSeries.pdf::p1 Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective Xingjian Wu, Xiangfei Qiu, Hanyin Cheng, Zhengyu Li, Jilin Hu, Chenjuan Guo, Bin Yang\n",
      "Sample image item: ImageItem(item_id='figure1.png', path='data/images/figure1.png', caption='Caption: figure1. Content: — al primal — Al primal aceaterated = Gn ww w Sewn Ai proximal accelerated Yeap 173 dl 13 proximal 10 2 — ti ait a7 pram eto, acclraied E E 3 3 — apina se] Sop act oe = Fad acatertes Bi roxas Bi proximal acaerted age | — 13 primal 27 da, 13 proximal ot 12 prima, 12 dl Xo prima, 12 proximal 1 dua, 172 proninal Waa, 37 proxi, acti ” ° 25 50 5 100 Ds 150 v5 200 * 100 125 150 15, 200 erate verte — — ai prima primal accelerated Sl primal accderted ar = as Tam accented BE sceserted we — all proximal ae — all proximal 2 proximal accelerated Ai prorina accelerated 113 primal, 3 dl 13 proximal 1 prima, 173 dl 13 proximal 112 primal, 12 dal 12 prima, 12 dal 1 = ar prima, 12 procs wo Ye md. 17 proximal — i aia 12 prima Ye ia. 172 primal - dia 2 prima, accra | ie Bia pron, cculemtes got 2 ot & & 3 3 os wo io wo woe toe oa io sO verte iemte Figure 1: Relative error of the consensus plan iterates for different configurations of the mix of agents and learning rates: pp = pa = px = 0.1 (top left), pp = pa = pr = 1 (top right), pp = pr = 10, pa = 1 (bottom left), pp = px = 50, pa = 1 (bottom right).')\n",
      "\n",
      "=== Deliverable: Extracted PDF Chunk ===\n",
      "Chunk ID:   doc1_TimeSeries.pdf::p1\n",
      "Source Doc: doc1_TimeSeries.pdf\n",
      "Page Num:   1\n",
      "Text Content (First 300 chars):\n",
      "Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective Xingjian Wu, Xiangfei Qiu, Hanyin Cheng, Zhengyu Li, Jilin Hu, Chenjuan Guo, Bin Yang∗ East China Normal University {xjwu,xfqiu,hycheng,lizhengyu}@stu.ecnu.edu.cn, {jlhu,cjguo,byang}@dase.ecnu.edu.cn Abstr...\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== Deliverable: Extracted Image Evidence ===\n",
      "Image ID: figure1.png\n",
      "Path:     data/images/figure1.png\n",
      "--------------------\n",
      "Full Evidence Text (Caption + OCR):\n",
      "Caption: figure1. Content: — al primal — Al primal aceaterated = Gn ww w Sewn Ai proximal accelerated Yeap 173 dl 13 proximal 10 2 — ti ait a7 pram eto, acclraied E E 3 3 — apina se] Sop act oe = Fad acatertes Bi roxas Bi proximal acaerted age | — 13 primal 27 da, 13 proximal ot 12 prima, 12 dl Xo prima, 12 proximal 1 dua, 172 proninal Waa, 37 proxi, acti ” ° 25 50 5 100 Ds 150 v5 200 * 100 125 150 15, 200 erate verte — — ai prima primal accelerated Sl primal accderted ar = as Tam accented BE sceserted we — all proximal ae — all proximal 2 proximal accelerated Ai prorina accelerated 113 primal, 3 dl 13 proximal 1 prima, 173 dl 13 proximal 112 primal, 12 dal 12 prima, 12 dal 1 = ar prima, 12 procs wo Ye md. 17 proximal — i aia 12 prima Ye ia. 172 primal - dia 2 prima, accra | ie Bia pron, cculemtes got 2 ot & & 3 3 os wo io wo woe toe oa io sO verte iemte Figure 1: Relative error of the consensus plan iterates for different configurations of the mix of agents and learning rates: pp = pa = px = 0.1 (top left), pp = pa = pr = 1 (top right), pp = pr = 10, pa = 1 (bottom left), pp = px = 50, pa = 1 (bottom right).\n"
     ]
    }
   ],
   "source": [
    "# Track B (Recommended)\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "@dataclass\n",
    "class TextChunk:\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    page_num: int\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class ImageItem:\n",
    "    item_id: str\n",
    "    path: str\n",
    "    caption: str  # simple text to make image retrieval runnable\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s or \"\"\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def extract_pdf_pages(pdf_path: str) -> List[TextChunk]:\n",
    "    doc_id = os.path.basename(pdf_path)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    out: List[TextChunk] = []\n",
    "    for i in range(len(doc)):\n",
    "        page = doc.load_page(i)\n",
    "        text = clean_text(page.get_text(\"text\"))\n",
    "        if text:\n",
    "            out.append(TextChunk(\n",
    "                chunk_id=f\"{doc_id}::p{i+1}\",\n",
    "                doc_id=doc_id,\n",
    "                page_num=i+1,\n",
    "                text=text\n",
    "            ))\n",
    "    return out\n",
    "\n",
    "def load_images_track_b(fig_dir: str) -> List[ImageItem]:\n",
    "    items: List[ImageItem] = []\n",
    "    print(f\"Scanning images in {fig_dir} with OCR...\")\n",
    "\n",
    "    for p in sorted(glob.glob(os.path.join(fig_dir, \"*.*\"))):\n",
    "        base = os.path.basename(p)\n",
    "\n",
    "        # 1. Generate Caption (Filename based)\n",
    "        simple_caption = os.path.splitext(base)[0].replace(\"_\", \" \")\n",
    "\n",
    "        # 2. Run OCR (Tesseract) to get text inside the image\n",
    "        try:\n",
    "            image = Image.open(p)\n",
    "            ocr_text = pytesseract.image_to_string(image).strip()\n",
    "            # Clean up OCR noise (optional)\n",
    "            ocr_text = re.sub(r\"\\s+\", \" \", ocr_text)\n",
    "        except Exception as e:\n",
    "            print(f\"OCR Failed for {base}: {e}\")\n",
    "            ocr_text = \"\"\n",
    "\n",
    "        # 3. Combine for Evidence (Track B Requirement)\n",
    "        # evidence_text = Caption + OCR\n",
    "        final_text = f\"Caption: {simple_caption}. Content: {ocr_text}\"\n",
    "\n",
    "        items.append(ImageItem(item_id=base, path=p, caption=final_text))\n",
    "\n",
    "    return items\n",
    "\n",
    "# Run ingestion\n",
    "page_chunks: List[TextChunk] = []\n",
    "for p in pdfs:\n",
    "    page_chunks.extend(extract_pdf_pages(p))\n",
    "\n",
    "image_items = load_images_track_b(FIG_DIR)\n",
    "\n",
    "print(\"Total text chunks:\", len(page_chunks))\n",
    "print(\"Total images:\", len(image_items))\n",
    "print(\"Sample text chunk:\", page_chunks[0].chunk_id, page_chunks[0].text[:180])\n",
    "print(\"Sample image item:\", image_items[0])\n",
    "\n",
    "# --- Deliverable Output ---\n",
    "\n",
    "print(\"\\n=== Deliverable: Extracted PDF Chunk ===\")\n",
    "if page_chunks:\n",
    "    chunk = page_chunks[0]\n",
    "    print(f\"Chunk ID:   {chunk.chunk_id}\")\n",
    "    print(f\"Source Doc: {chunk.doc_id}\")\n",
    "    print(f\"Page Num:   {chunk.page_num}\")\n",
    "    print(f\"Text Content (First 300 chars):\\n{chunk.text[:300]}...\")\n",
    "else:\n",
    "    print(\"❌ No PDF chunks found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\n=== Deliverable: Extracted Image Evidence ===\")\n",
    "if image_items:\n",
    "    item = image_items[0]\n",
    "    print(f\"Image ID: {item.item_id}\")\n",
    "    print(f\"Path:     {item.path}\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Full Evidence Text (Caption + OCR):\\n{item.caption}\")\n",
    "    # Note: item.caption now holds \"Caption: [filename]. Content: [OCR Text]\"\n",
    "else:\n",
    "    print(\"❌ No images found.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b108aa",
   "metadata": {
    "id": "d9b108aa"
   },
   "source": [
    "# 4) Mini Gold Set (Q1–Q5) — Required\n",
    "\n",
    "Create **5 project-relevant queries** and define a simple evidence rubric.\n",
    "\n",
    "- **Q1–Q3:** typical project queries (answerable using evidence)\n",
    "- **Q4:** multimodal evidence query (table/figure heavy, OCR/captions should help)\n",
    "- **Q5:** missing-evidence or ambiguous query (must trigger safe behavior)\n",
    "\n",
    "For each query, define:\n",
    "- `gold_evidence_ids`: list of evidence identifiers that are relevant (doc_id/page/fig id)\n",
    "- `answer_criteria`: 1–2 bullets\n",
    "- `citation_format`: how you will cite (e.g., `[Doc1 p3]`, `[fig2]`)\n",
    "\n",
    "This enables **consistent evaluation** and makes logging meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c131bc70",
   "metadata": {
    "id": "c131bc70"
   },
   "outputs": [],
   "source": [
    "# Task: Populate a mini gold set for monitoring and ablation\n",
    "# Evidence identifiers use doc_ids from the demo corpus (file basenames under ./data/docs).\n",
    "\n",
    "mini_gold = [\n",
    "    # ── Q1  (typical project query – doc1: SRSNet) ───────────────────────────\n",
    "    {\n",
    "        \"query_id\": \"Q1\",\n",
    "        \"question\": (\n",
    "            \"What is the Selective Representation Space (SRS) module in SRSNet \"\n",
    "            \"and how does it differ from conventional adjacent patching?\"\n",
    "        ),\n",
    "        \"gold_evidence_ids\": [\n",
    "            \"doc1_TimerSeries.pdf\",   # main paper text describing SRS\n",
    "            \"figure8.png\",            # Figure 2 – overall SRS pipeline\n",
    "            \"figure9.png\",            # Figure 3 – detailed architecture\n",
    "        ],\n",
    "        \"answer_criteria\": [\n",
    "            \"Explains that SRS uses Selective Patching (gradient-based, learnable patch selection with stride=1 scanning) \"\n",
    "            \"instead of fixed-length adjacent patching\",\n",
    "            \"Mentions the Dynamic Reassembly step that re-orders the selected patches based on learned scores\",\n",
    "            \"Notes the Adaptive Fusion that integrates embeddings from both conventional and selective patching\",\n",
    "            \"Includes at least one citation to a document or figure\",\n",
    "        ],\n",
    "        \"citation_format\": \"[doc1_TimerSeries.pdf] or [figure8.png] / [figure9.png]\",\n",
    "    },\n",
    "\n",
    "    # ── Q2  (typical project query – doc2: ReMindRAG) ────────────────────────\n",
    "    {\n",
    "        \"query_id\": \"Q2\",\n",
    "        \"question\": (\n",
    "            \"How does ReMindRAG's memory replay mechanism improve retrieval \"\n",
    "            \"for similar or repeated queries?\"\n",
    "        ),\n",
    "        \"gold_evidence_ids\": [\n",
    "            \"doc2_ReMindRAG.pdf\",     # main paper text\n",
    "            \"figure3.png\",            # Figure 1 – overall workflow showing memorize/replay\n",
    "            \"figure7.png\",            # Figure 5 – memory replay under Same Query setting\n",
    "        ],\n",
    "        \"answer_criteria\": [\n",
    "            \"Describes the enhance/penalize edge-weight update after the first query\",\n",
    "            \"Explains that on a subsequent similar/same query the system reuses the memorized traversal path \"\n",
    "            \"(skipping full LLM-guided KG traversal)\",\n",
    "            \"Includes at least one citation\",\n",
    "        ],\n",
    "        \"citation_format\": \"[doc2_ReMindRAG.pdf] or [figure3.png] / [figure7.png]\",\n",
    "    },\n",
    "\n",
    "    # ── Q3  (typical project query – doc3: Consensus Planning Problem) ───────\n",
    "    {\n",
    "        \"query_id\": \"Q3\",\n",
    "        \"question\": (\n",
    "            \"What real-world applications of the Consensus Planning Problem (CPP) \"\n",
    "            \"are described, and what agent interfaces does each application use?\"\n",
    "        ),\n",
    "        \"gold_evidence_ids\": [\n",
    "            \"doc3_CPP.pdf\",           # main paper text\n",
    "            \"figure2.png\",            # Table 1 – examples of consensus problems\n",
    "        ],\n",
    "        \"answer_criteria\": [\n",
    "            \"Lists at least three applications (Fullness Optimization, Throughput Coordination, \"\n",
    "            \"Transportation Optimization, Arrivals & Throughput Coordination)\",\n",
    "            \"States the interface type (primal / dual / proximal) used by the agents in each application\",\n",
    "            \"Includes a citation to Table 1 or the document\",\n",
    "        ],\n",
    "        \"citation_format\": \"[doc3_CPP.pdf] or [figure2.png, Table 1]\",\n",
    "    },\n",
    "\n",
    "    # ── Q4  (multimodal / table-heavy query – doc2 Table 1 + doc1 Table 2) ──\n",
    "    {\n",
    "        \"query_id\": \"Q4\",\n",
    "        \"question\": (\n",
    "            \"According to Table 1 in the ReMindRAG paper, what is the Multi-Hop QA \"\n",
    "            \"accuracy of ReMindRAG with the Deepseek-V3 backbone, and how does it \"\n",
    "            \"compare to HippoRAG2 on the same task and backbone?\"\n",
    "        ),\n",
    "        \"gold_evidence_ids\": [\n",
    "            \"doc2_ReMindRAG.pdf\",\n",
    "            \"figure6.png\",            # Table 1 – Effectiveness Performance\n",
    "        ],\n",
    "        \"answer_criteria\": [\n",
    "            \"Extracts the correct numeric value for ReMindRAG Multi-Hop QA / Deepseek-V3: 79.38%\",\n",
    "            \"Extracts HippoRAG2 Multi-Hop QA / Deepseek-V3: 64.95%\",\n",
    "            \"Notes that ReMindRAG outperforms HippoRAG2 by ~14.43 percentage points\",\n",
    "            \"Includes a citation to Table 1 or figure6\",\n",
    "        ],\n",
    "        \"citation_format\": \"[doc2_ReMindRAG.pdf, Table 1] or [figure6.png]\",\n",
    "    },\n",
    "\n",
    "    # ── Q5  (missing-evidence / ambiguous query – must trigger safe behavior)\n",
    "    {\n",
    "        \"query_id\": \"Q5\",\n",
    "        \"question\": (\n",
    "            \"What reinforcement learning reward function does SRSNet use to train \"\n",
    "            \"the Selective Patching scorer?\"\n",
    "        ),\n",
    "        \"gold_evidence_ids\": [\"N/A\"],  # SRSNet does NOT use RL; the scorer is gradient-based\n",
    "        \"answer_criteria\": [\n",
    "            \"Returns a missing-evidence / 'insufficient information' response\",\n",
    "            \"Does NOT hallucinate a reinforcement learning component – \"\n",
    "            \"SRSNet's scorer is gradient-based (Gumbel-Softmax), not RL-based\",\n",
    "            \"Optionally clarifies that the actual mechanism is gradient-based, citing the document\",\n",
    "        ],\n",
    "        \"citation_format\": \"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "pd.DataFrame(mini_gold)[[\"query_id\",\"question\",\"gold_evidence_ids\"]]\n",
    "\n",
    "\n",
    "\n",
    "# Optional multimodal query (image evidence via caption surrogate)\n",
    "mini_gold.append(   # ── Q6  (multimodal query – image evidence via caption surrogate) ────────\n",
    "    {\n",
    "        \"query_id\": \"Q6\",\n",
    "        \"question\": (\n",
    "            \"What are the key stages shown in the ReMindRAG overall workflow diagram?\"\n",
    "        ),\n",
    "        \"gold_evidence_ids\": [\"img::figure3.png\"],  # Figure 1 – ReMindRAG overall workflow\n",
    "        \"answer_criteria\": [\n",
    "            \"Mentions KG construction from documents (Build stage: Document → Chunks → KG)\",\n",
    "            \"Mentions LLM-guided KG traversal with seed node selection and path expansion\",\n",
    "            \"Mentions the Enhance/Penalize edge-weight update (memory) after the first query\",\n",
    "            \"Mentions the fast retrieval shortcut for subsequent similar/same queries\",\n",
    "        ],\n",
    "        \"citation_format\": \"[img::figure3.png]\",\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rc7GAupjbK43",
   "metadata": {
    "id": "Rc7GAupjbK43"
   },
   "outputs": [],
   "source": [
    "# Task: Mini gold set (evidence IDs) for evaluation\n",
    "# Evidence IDs refer to demo files under ./data/docs (use basenames). Image evidence uses the prefix img::\n",
    "import pandas as pd\n",
    "\n",
    "mini_gold = [\n",
    "    {\n",
    "        'query_id': 'Q1',\n",
    "        'question': 'What is the Selective Representation Space (SRS) module in SRSNet and how does it differ from conventional adjacent patching?',\n",
    "        'gold_evidence_ids': ['doc1_TimerSeries.pdf']\n",
    "    },\n",
    "    {\n",
    "        'query_id': 'Q2',\n",
    "        'question': 'How does ReMindRAG\\'s memory replay mechanism improve retrieval for similar or repeated queries?',\n",
    "        'gold_evidence_ids': ['doc2_ReMindRAG.pdf']\n",
    "    },\n",
    "    {\n",
    "        'query_id': 'Q3',\n",
    "        'question': 'What real-world applications of the Consensus Planning Problem are described, and what agent interfaces does each application use?',\n",
    "        'gold_evidence_ids': ['doc3_CPP.pdf']\n",
    "    },\n",
    "    {\n",
    "        'query_id': 'Q4',\n",
    "        'question': 'According to Table 1 in the ReMindRAG paper, what is the Multi-Hop QA accuracy of ReMindRAG with the Deepseek-V3 backbone compared to HippoRAG2?',\n",
    "        'gold_evidence_ids': ['img::figure6.png']\n",
    "    },\n",
    "    {\n",
    "        'query_id': 'Q5',\n",
    "        'question': 'What reinforcement learning reward function does SRSNet use to train the Selective Patching scorer?',\n",
    "        'gold_evidence_ids': ['N/A']\n",
    "    },\n",
    "]\n",
    "\n",
    "pd.DataFrame(mini_gold)[['query_id','question','gold_evidence_ids']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3e87e",
   "metadata": {
    "id": "6da3e87e"
   },
   "source": [
    "# 5) Retrieval + Answer Function (Reuse Lab 3)\n",
    "\n",
    "Below is a **baseline TF‑IDF retriever** so this notebook is runnable.\n",
    "Replace with your Lab-3 retrieval stack:\n",
    "- dense (SentenceTransformers + FAISS/Chroma)\n",
    "- sparse (BM25)\n",
    "- hybrid fusion\n",
    "- optional reranking\n",
    "\n",
    "### Required output contract (recommended)\n",
    "Your retrieval function should return a list of evidence items:\n",
    "- `chunk_id` or `doc_id`\n",
    "- `source`\n",
    "- `score`\n",
    "- `citation_tag` (e.g., `[Doc1 p3]`, `[fig2]`)\n",
    "- `text` (the evidence text shown to users)\n",
    "\n",
    "Your answer function must enforce:\n",
    "- **Citations for claims**\n",
    "- If missing evidence: **return exactly**  \n",
    "  `Not enough evidence in the retrieved context.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1f60c59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1f60c59",
    "outputId": "fc50fafe-99b9-4a42-dd41-9d789fa098a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top evidence: 07_numeric_table 0.0\n",
      "Answer: Not enough evidence in the retrieved context.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Build a simple TF-IDF index over documents (demo baseline)\n",
    "corpus = [d[\"text\"] for d in documents]\n",
    "doc_ids = [d[\"doc_id\"] for d in documents]\n",
    "sources = [d[\"source\"] for d in documents]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "def retrieve_tfidf(question: str, top_k: int = 5):\n",
    "    q = vectorizer.transform([question])\n",
    "    sims = cosine_similarity(q, X).ravel()\n",
    "    idxs = np.argsort(-sims)[:top_k]\n",
    "    evidence = []\n",
    "    for rank, i in enumerate(idxs):\n",
    "        evidence.append({\n",
    "            \"chunk_id\": doc_ids[i],\n",
    "            \"source\": sources[i],\n",
    "            \"score\": float(sims[i]),\n",
    "            \"citation_tag\": f\"[{doc_ids[i]}]\",\n",
    "            \"text\": corpus[i][:800]  # truncate for UI\n",
    "        })\n",
    "    return evidence\n",
    "\n",
    "MISSING_EVIDENCE_MSG = \"Not enough evidence in the retrieved context.\"\n",
    "\n",
    "def generate_answer_stub(question: str, evidence: list):\n",
    "    \"\"\"Replace with your LLM/VLM generation.\n",
    "    For this template we produce a simple grounded response.\n",
    "    \"\"\"\n",
    "    if not evidence or max(e.get(\"score\", 0.0) for e in evidence) < 0.05:\n",
    "        return MISSING_EVIDENCE_MSG\n",
    "\n",
    "    # Minimal grounded \"answer\" example: summarize top evidence\n",
    "    top = evidence[0]\n",
    "    answer = (\n",
    "        f\"Based on the retrieved evidence {top['citation_tag']}, \"\n",
    "        f\"the system should ground its response in retrieved context and cite sources. \"\n",
    "        f\"If evidence is missing, it must respond with: '{MISSING_EVIDENCE_MSG}'. \"\n",
    "        f\"{top['citation_tag']}\"\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "# Quick test\n",
    "test_q = mini_gold[0][\"question\"]\n",
    "ev = retrieve_tfidf(test_q, top_k=3)\n",
    "print(\"Top evidence:\", ev[0][\"chunk_id\"], ev[0][\"score\"])\n",
    "print(\"Answer:\", generate_answer_stub(test_q, ev))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l2eqmYFwgaB4",
   "metadata": {
    "id": "l2eqmYFwgaB4"
   },
   "source": [
    "## Fixed-size Chunking Strategy (Sliding Window Chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13srE4FigwxC",
   "metadata": {
    "id": "13srE4FigwxC"
   },
   "outputs": [],
   "source": [
    "# Chunking knobs (for fixed-size chunking ablation)\n",
    "CHUNK_SIZE    = 900   # characters per chunk\n",
    "CHUNK_OVERLAP = 150   # overlap characters\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "BixULTsmfb8N",
   "metadata": {
    "id": "BixULTsmfb8N"
   },
   "outputs": [],
   "source": [
    "def extract_fixed_size_chunks(pdf_path: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[TextChunk]:\n",
    "    doc_id = os.path.basename(pdf_path)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += clean_text(page.get_text(\"text\")) + \" \"\n",
    "\n",
    "    # Sliding window slicing\n",
    "    chunks = []\n",
    "    for i in range(0, len(full_text), chunk_size - overlap):\n",
    "        window = full_text[i : i + chunk_size]\n",
    "        if len(window) > 50: # Filter tiny chunks\n",
    "            chunks.append(TextChunk(\n",
    "                chunk_id=f\"{doc_id}::span{i}-{i+len(window)}\",\n",
    "                doc_id=doc_id,\n",
    "                page_num=0, # Logical chunk, not page bound\n",
    "                text=window\n",
    "            ))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O6Wgzw2BhBLv",
   "metadata": {
    "id": "O6Wgzw2BhBLv"
   },
   "source": [
    "## Retrieval (TF‑IDF)\n",
    "We build two TF‑IDF indexes:\n",
    "- One over **PDF text chunks**\n",
    "- One over **image captions**\n",
    "\n",
    "Retrieval returns the top‑k results with similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "AW4G3CtMhMqz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AW4G3CtMhMqz",
    "outputId": "3635bd87-d003-42b4-d356-8bdee82536c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexes built.\n",
      "--- Text Index (98 items) ---\n",
      "ID 0: Enhancing Time Series Forecasting through Selectiv...\n",
      "ID 1: Contextual Series Forecasting Horizon Loss of Info...\n",
      "ID 2: the rapid progress in machine learning technologie...\n",
      "ID 3: Patch Embedding ① × 1 ② × 1 ③ × 2 ④ × 2 ① ② ③ ③ ④ ...\n",
      "ID 4: Selective Patching Dynamic Reassembly … scan with ...\n",
      "\n",
      "--- Image Index (15 items) ---\n",
      "ID 0: Caption: figure1. Content: — al primal — Al primal aceaterated = Gn ww w Sewn Ai proximal accelerated Yeap 173 dl 13 proximal 10 2 — ti ait a7 pram eto, acclraied E E 3 3 — apina se] Sop act oe = Fad acatertes Bi roxas Bi proximal acaerted age | — 13 primal 27 da, 13 proximal ot 12 prima, 12 dl Xo prima, 12 proximal 1 dua, 172 proninal Waa, 37 proxi, acti ” ° 25 50 5 100 Ds 150 v5 200 * 100 125 150 15, 200 erate verte — — ai prima primal accelerated Sl primal accderted ar = as Tam accented BE sceserted we — all proximal ae — all proximal 2 proximal accelerated Ai prorina accelerated 113 primal, 3 dl 13 proximal 1 prima, 173 dl 13 proximal 112 primal, 12 dal 12 prima, 12 dal 1 = ar prima, 12 procs wo Ye md. 17 proximal — i aia 12 prima Ye ia. 172 primal - dia 2 prima, accra | ie Bia pron, cculemtes got 2 ot & & 3 3 os wo io wo woe toe oa io sO verte iemte Figure 1: Relative error of the consensus plan iterates for different configurations of the mix of agents and learning rates: pp = pa = px = 0.1 (top left), pp = pa = pr = 1 (top right), pp = pr = 10, pa = 1 (bottom left), pp = px = 50, pa = 1 (bottom right). (File: figure1.png)\n",
      "ID 1: Caption: figure10. Content: Table 2: Multivariate forecasting average results with forecasting horizons F' € {96, 192, 336, 720} for the datasets. Red: the best, Blue: the 2nd best. Full results are available in Table|8|of Appendix (Al Datasets | ETThi | ETTh | ETTm | ETTm2 | Weather | Electricity | | Solar | Traffic Metrics | mse mae | mse mac | mse mae | mse mae | mse mac | mse mae | mse mac | mse mae FEDformer [2022] | 0.433 0.454 | 0.406 0.438 | 0.567 0.519 | 0.335 0.380 | 0.312 0.356 | 0.219 0.330 | 0.641 0.628 | 0.620 0.382 Stationary [2022] | 0.667 0.568 | 0.377 0.419 | 0.531 0.472 | 0.384 0.390 | 0.287 0.310 | 0.194 0.295 | 0.390 0.387 | 0.622 0.340 DLinear [2023] | 0.430 0.443 | 0.470 0.468 | 0.356 0.378 | 0.259 0.324 | 0.242 0.295 | 0.167 0.264 | 0.224 0.286 | 0.418 0.287 TimesNet [2023] | 0.468 0.459 | 0.390 0.417 | 0.408 0.415 | 0.292 0.331 | 0.255 0.282 | 0.190 0.284 | 0.211 0.281 | 0.617 0.327 Crossformer [2023] | 0.439 0.461 | 0.894 0.680 | 0.464 0.456 | 0.501 0.505 | 0.232 0.294 | 0.171 0.263 | 0.205 0.232 | 0.522 0.282 PatchTST [2023] | 0.419 0.436 | 0.351 0.395 | 0.349 0.381 | 0.256 0.314 | 0.224 0.262 | 0.171 0.270 | 0.200 0.284 | 0.397 0.275. TimeMixer [2024] | 0.427 (0.441 | 0.347 0.394 | 0.356 0.380 | 0.257 0.318 | 0.225 0.263 | 0.185 0.284 | 0.203 0.261 | 0.410 0.279 iTransformer [2024] | 0.440 0.445 | 0.359 0.396 | 0.347 0.378 | 0.258 0.318 | 0.232 0.270 | 0.163 0.258 | 0.202 0.260 | 0.397 0.281 Amplifier [2025] | 0.421 0.433 | 0.356 0.402 | 0.353 0.379 | 0.256 0.318 | 0.223 0.264 | 0.163 0.256 | 0.202 0.256 | 0.417 0.290 TimeKAN [2025] | 0.409 0.427 | 0.350 0.397 | 0.344 0.380 | 0.260 0.318 | 0.226 0.268 | 0.164 0.258 | 0.198 0.263 | 0.420 0.286 SRSNet | 0.404 0.424 | 0.334 0.385 | 0.351 0.378 | 0.252 0.314 | 0.226 0.266 | 0.161 0.254 | 0.183 0.239 | 0.392 0.270 (File: figure10.png)\n",
      "ID 2: Caption: figure11. Content: Table 1: Statistics of datasets. Dataset. Domain Frequency Lengths Dim Split Description ETThI Electricity 1 hour 14,400 Power transformer 1, comprising seven indicators such as oil temperature and useful load ETTh2 — Electricity 1 hour 14,400 Power transformer 2, comprising seven indicators such as oil temperature and useful load ETTml Electricity 15 mins 57,600 Power transformer 1, comprising seven indicators such as oil temperature and useful load ETTm2 Electricity 15 mins 57,600 Power transformer 2, comprising seven indicators such as oil temperature and useful load Weather Environment — 10 mins 52,696 Recorded every for the whole year 2020, which contains 21 meteorological indicators Electricity Electricity 1 hour 26,304 Electricity records the electricity consumption in KWh every | hour from 2012 to 2014 Solar Energy 10 mins 52,560 Solar production records collected from 137 PV plants in Alabama Traffic Traffic 1 hour 17,544 Road occupancy rates measured by 862 sensors on San Francisco Bay area freeways (File: figure11.png)\n",
      "ID 3: Caption: figure12. Content: Table 5: Efficiency comparison between SRSNet Table 6: Efficiency analysis of the SRS mod- and other baselines on ETThI and Solar datasets _ule (the same settings as in Table|5). The Max with look-back length equals 512, forecasting GPU Memory (MB), Inference Time (s) per horizon equals 720, and batch size equals 32. _ batch, Training Time (s) per batch, and Multiply- The Max GPU Memory (MB) and Training Time | Accumulate Operations (MACs) are reported as (s) per batch are reported as the main metrics. the main metrics. Datasets I ETThl I Solar Datasets | Variants | Memory (MB) | Inference (s) | Training (s) | MACs (G) Meies | Memory (MB) | ‘raining Time(s) | Memory (MB) | Training Time () PatchTST 2,837 5.076 5.131 16.214 Linear |_Dlinear_ | 828 | 12 | 85s | 15.66 = +SRS 2,907 5.722 5.163 16.905 | Amptifer | 596 | 178 175 1438 E Overhead +2.47% +12.73% +12.31% | 44.26% CNN | TimesNet_| 2846 | 495 |_| 1812.46 S| Crossformer 4011 32.613 56.280 [_FeDformer | 8190 | 83_—| SSIS +SRS 4,159 5031 35.276 56.625, | Stationary | 18386 | 4022~~«|~~«BS2)_—«|~SCS*«S OT Overhead $3.69% +10.21% | 48.17% | 40.61% Transformer | Crossformer | 3.976 | 17.13 | 16.375 | 205.60 PatchTST 27,822.08 34.231 88.714 600.261 | PaichTsT [1408 [2a ~S«*S~Si TT _«|SS~«iST6O +SRS 29,767.68 95.200 101.981 | 613.790 [ilransformer|_722~«dY”~C(“‘é «YOON | 2066 z Overhead +6.99% +13.02% | +14.95% | 42.25% | TimeMiver | 1304 | 749 | 20,002 | TOTS % | Crossformer 17,355 79.031 82.472 61.822, MiP | TimeKAN | 1456 | <550~—~«(~SCSO)—=«dY~C*«i 4SRS 18,978 36.674 90.268 62.174 [SRSNat| 1012 | 227 | a0 | S189 Overhead 49.35% 49.67% 49.45% | 40.57% (File: figure12.png)\n",
      "ID 4: Caption: figure13. Content: —e ETThL a ETTm2 © Solar + Traffic ee +—__+—___+—__+ 0.45. 0.40. —e ETThL 0.35 = ETTm2 —* Solar 0.30 te Traffic 0.25 0.20 Sj —e ETThL = ETTm2 -e Solar MN MSE Ey 0.40. 0.35. —e ETThL a ETTm2 0.30 —* Solar 0.25 0.20 é 16 24 32 Patch Size (a) Patch Size 96 512 356 720 Look Back Window (b) Look Back Window T z 3 4 Hidden Layer (c) Hidden Layer 32 64 128 256 Hidden Dimension (d) Hidden Dimension Figure 4: Parameter sensitivity studies of main hyper-parameters in SRSNet. (File: figure13.png)\n"
     ]
    }
   ],
   "source": [
    "def build_tfidf_index_text(chunks: List[TextChunk]):\n",
    "    corpus = [c.text for c in chunks]\n",
    "    vec = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "    X = vec.fit_transform(corpus)\n",
    "    X = normalize(X)\n",
    "    return vec, X\n",
    "\n",
    "def build_tfidf_index_images(items: List[ImageItem]):\n",
    "    corpus = [it.caption for it in items]\n",
    "    vec = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "    X = vec.fit_transform(corpus)\n",
    "    X = normalize(X)\n",
    "    return vec, X\n",
    "\n",
    "text_vec, text_X = build_tfidf_index_text(page_chunks)\n",
    "img_vec, img_X = build_tfidf_index_images(image_items)\n",
    "\n",
    "def tfidf_retrieve(query: str, vec: TfidfVectorizer, X, top_k: int = 5):\n",
    "    q = vec.transform([query])\n",
    "    q = normalize(q)\n",
    "    scores = (X @ q.T).toarray().ravel()\n",
    "    idx = np.argsort(-scores)[:top_k]\n",
    "    return [(int(i), float(scores[i])) for i in idx]\n",
    "\n",
    "print(\"✅ Indexes built.\")\n",
    "\n",
    "# Inspect built indexes by listing first 5 as a sample\n",
    "print(f\"--- Text Index ({len(page_chunks)} items) ---\")\n",
    "for i, chunk in enumerate(page_chunks[:5]):  # Print first 5 as a sample\n",
    "    # Assuming 'chunk' has a 'source_doc' or similar attribute, otherwise just print text\n",
    "    preview = chunk.text[:50].replace(\"\\n\", \" \") + \"...\"\n",
    "    print(f\"ID {i}: {preview}\")\n",
    "\n",
    "print(f\"\\n--- Image Index ({len(image_items)} items) ---\")\n",
    "for i, item in enumerate(image_items[:5]):\n",
    "    print(f\"ID {i}: {item.caption} (File: {item.item_id})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vcSPijTUheaA",
   "metadata": {
    "id": "vcSPijTUheaA"
   },
   "source": [
    "## Build Dense Retrieval and Figure Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "g_2vGqovhnjd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_2vGqovhnjd",
    "outputId": "5c58468c-7a8f-4b97-83bc-2236dd29d5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q sentence-transformers\n",
    "!pip install -q sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "PZSCEWuYiPkT",
   "metadata": {
    "id": "PZSCEWuYiPkT"
   },
   "outputs": [],
   "source": [
    "# Retrieval knobs\n",
    "TOP_K_TEXT     = 5    # candidate text chunks\n",
    "TOP_K_IMAGES   = 3    # candidate images (based on captions/filenames)\n",
    "TOP_K_EVIDENCE = 8    # final evidence items used in the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "JXpTXDJehqSU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "759a68f87a8e4a5b8df02124c2c82152",
      "7c3000dbb7bf4bfcb2bcc91b519c52ce",
      "e52199b62c7645c682a47266a6dc64f3",
      "46510f1cbcfc4bf693b660741b16c364",
      "ab4194abdb304161bc2608bc67fa2665",
      "3bc95c580d384928af685d74be7c34d2",
      "9bb701e8fe5f4378be12c6c609f9325c",
      "74cca1822afc4e449679c52605e5a0f6",
      "79980d5bb29547129550999ec6f7ee21",
      "d7c9b5ca76364ee9a3c9cb73117a01e1",
      "564d153289b842f69b04526ae8cc4ba6"
     ]
    },
    "id": "JXpTXDJehqSU",
    "outputId": "5abafa64-f71d-4c8d-d51c-95ed88a00739"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759a68f87a8e4a5b8df02124c2c82152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dense Index built with 98 vectors.\n",
      "✅ Approach 1 (Captions): Indexed 15 images via text.\n",
      "Text Dictionary Size: 5381\n",
      "Image Dictionary Size: 862\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create embeddings\n",
    "corpus_text = [c.text for c in page_chunks]\n",
    "# Remove convert_to_tensor=True so we get a NumPy array for FAISS\n",
    "corpus_embeddings = model.encode(corpus_text)\n",
    "\n",
    "# Build FAISS Index\n",
    "d = corpus_embeddings.shape[1]  # Dimension of embeddings (e.g., 384)\n",
    "index_dense = faiss.IndexFlatL2(d) # L2 distance (Euclidean)\n",
    "index_dense.add(corpus_embeddings)\n",
    "\n",
    "print(f\"✅ Dense Index built with {index_dense.ntotal} vectors.\")\n",
    "\n",
    "# Embed the captions from your image_items list\n",
    "corpus_caption = [item.caption for item in image_items]\n",
    "caption_embeddings = model.encode(corpus_caption, convert_to_tensor=False)\n",
    "\n",
    "# Build FAISS Index for Image Captions\n",
    "d_cap = caption_embeddings.shape[1] # Dimension = 384\n",
    "index_captions = faiss.IndexFlatL2(d_cap)\n",
    "index_captions.add(caption_embeddings)\n",
    "\n",
    "print(f\"✅ Approach 1 (Captions): Indexed {index_captions.ntotal} images via text.\")\n",
    "\n",
    "def dense_retrieve(query, top_k=TOP_K_TEXT):\n",
    "    # Encode query to numpy. Wrap in list [query] to ensure (1, d) shape.\n",
    "    query_emb = model.encode([query])\n",
    "\n",
    "    # Search FAISS\n",
    "    distances, indices = index_dense.search(query_emb, top_k)\n",
    "\n",
    "    # Return indices\n",
    "    return [(int(idx), float(dist)) for idx, dist in zip(indices[0], distances[0])]\n",
    "\n",
    "def retrieve_images_by_caption(query: str, top_k=TOP_K_IMAGES):\n",
    "    # Embed query using the SAME text model\n",
    "    q_emb = model.encode([query])\n",
    "    distances, indices = index_captions.search(q_emb, top_k)\n",
    "\n",
    "    # Return matched ImageItems\n",
    "    results = []\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        if idx < 0: continue # FAISS returns -1 if not found\n",
    "        results.append((image_items[idx], float(dist)))\n",
    "    return results\n",
    "\n",
    "# Validation by checking vocabulary size\n",
    "print(f\"Text Dictionary Size: {len(text_vec.vocabulary_)}\")\n",
    "print(f\"Image Dictionary Size: {len(img_vec.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XfQmsfD_oWWs",
   "metadata": {
    "id": "XfQmsfD_oWWs"
   },
   "source": [
    "## Build evidence context\n",
    "We assemble a compact context string + list of image paths.\n",
    "\n",
    "**Guidelines for good context:**\n",
    "- Keep snippets short (100–300 chars)\n",
    "- Always include chunk IDs so you can cite evidence\n",
    "- Attach images that are likely relevant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "CaTyB7aqoanT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CaTyB7aqoanT",
    "outputId": "a91049c2-65d1-4b07-e3e8-287b26d7c000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEXT | doc1_TimeSeries.pdf::p2 | fused=0.500] Contextual Series Forecasting Horizon Loss of Information Out of distribution Disturbance from noise Changeable Periods Shifting Anomalies Adjacent Patching Selective Patching Changeable Periods Shifting Anomalies Retainment of periodicity In distribution Avoi\n",
      "[IMAGE | figure8.png | fused=0.500] caption=Caption: figure8. Content: Adjacent Patch Adaptive Fusion Patching Embeddin: selectins, o Patching | by] 5 rater mi ing Channel 2 fo A Adjacent Patch | Adaptive Fusion NN anna Patching YY Selective, a patching ee channel 3 Adjacent Patch Adaptive Fusion Patching Embeddin. YY Selecti Patch ing _,Dynamic_, Patch Reassembly Embedding Position Embedding Position Embedding || Position Embedding | | |Patch-based| models Figure 2: The overall pipeline of the SRS module. The multivariate time series is processed with Channel Independent strategy, the Selective Patching first adaptively chooses proper patches from all potential candidate patches. Then the Dynamic Reassembly dertermines the order of the selected patches. Both the Selective Patching and Dynamic Reassembly are gradient-based and learnable. Finally, the Adaptive Fusion integrates the embeddings from Conventional Patching and Dynamic Reassembly, adds the position embeddings to construct the final representations. The subsequent backbones can be used directly without changes, so that the SRS module is a modular plugin.\n",
      "[IMAGE | figure9.png | fused=0.310] caption=Caption: figure9. Content: Selective Patching “==> Dynamic Reassembly ==> Embedding selected indices (justanexample) = r>>>>~ Patch Embedding Fused with the Generate scores of all Scorer” embedding from patches for each sampling Generate scores of L S Adjacent Patching = v e - selected patches u f-----s E argmax & i\" | | BH Embedding | Legend | 3 3 Sort the patches | Eearmaclel 5 2 | P | Parts | & PatchNum =i & PatchNum =i Sete (stride = 1) (stride = 1) Representation Figure 3: The detailed architecture of the SRS module. The Selective Patching allows sampling with replacement. It scans all the potential patches with stride equals 1, generates n scores for each, then retrieves the patches with max scores in each sampling. Then the Dynamic Reassembly generates scores for selected patches, and sorts them based on the scores to determine the sequence. In the Embedding phase, both the embeddings from the Dynamic Reassembly and Conventional Patching are adaptively fused to form the representations.\n",
      "[TEXT | doc1_TimeSeries.pdf::p4 | fused=0.304] Patch Embedding ① × 1 ② × 1 ③ × 2 ④ × 2 ① ② ③ ③ ④ ④ ① × 2 ② × 3 ③ × 1 ③ × 4 ① ① ② ③ ③ ③ ③ ① ① ② ② ② ③ Selective Patching Dynamic Reassembly Adaptive Fusion Adaptive Fusion Adaptive Fusion Position Embedding Patch Embedding Adjacent Patching Patch-based models \n",
      "[TEXT | doc1_TimeSeries.pdf::p10 | fused=0.149] Figure 4b demonstrates the capability of SRSNet to fully utilize the historical information. As the length of Look Back Window extends, the performance becomes better. Figure 4c and Figure 4c show the influences of the hidden layer and hidden dimension in the \n",
      "[TEXT | doc1_TimeSeries.pdf::p8 | fused=0.049] 5.3 Ablation Study and Analysis To investigate the effectiveness of SRS, we conduct comprehensive experiments on four datasets from multiple domains with different lengths and numbers of variables: ETTh1, ETTm2, Solar, and Traffic. The experiments include two \n",
      "[TEXT | doc1_TimeSeries.pdf::p1 | fused=0.000] Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective Xingjian Wu, Xiangfei Qiu, Hanyin Cheng, Zhengyu Li, Jilin Hu, Chenjuan Guo, Bin Yang∗ East China Normal University {xjwu,xfqiu,hycheng,lizhengyu}@stu.ecnu.edu.cn, {\n",
      "[IMAGE | figure12.png | fused=0.000] caption=Caption: figure12. Content: Table 5: Efficiency comparison between SRSNet Table 6: Efficiency analysis of the SRS mod- and other baselines on ETThI and Solar datasets _ule (the same settings as in Table|5). The Max with look-back length equals 512, forecasting GPU Memory (MB), Inference Time (s) per horizon equals 720, and batch size equals 32. _ batch, Training Time (s) per batch, and Multiply- The Max GPU Memory (MB) and Training Time | Accumulate Operations (MACs) are reported as (s) per batch are reported as the main metrics. the main metrics. Datasets I ETThl I Solar Datasets | Variants | Memory (MB) | Inference (s) | Training (s) | MACs (G) Meies | Memory (MB) | ‘raining Time(s) | Memory (MB) | Training Time () PatchTST 2,837 5.076 5.131 16.214 Linear |_Dlinear_ | 828 | 12 | 85s | 15.66 = +SRS 2,907 5.722 5.163 16.905 | Amptifer | 596 | 178 175 1438 E Overhead +2.47% +12.73% +12.31% | 44.26% CNN | TimesNet_| 2846 | 495 |_| 1812.46 S| Crossformer 4011 32.613 56.280 [_FeDformer | 8190 | 83_—| SSIS +SRS 4,159 5031 35.276 56.625, | Stationary | 18386 | 4022~~«|~~«BS2)_—«|~SCS*«S OT Overhead $3.69% +10.21% | 48.17% | 40.61% Transformer | Crossformer | 3.976 | 17.13 | 16.375 | 205.60 PatchTST 27,822.08 34.231 88.714 600.261 | PaichTsT [1408 [2a ~S«*S~Si TT _«|SS~«iST6O +SRS 29,767.68 95.200 101.981 | 613.790 [ilransformer|_722~«dY”~C(“‘é «YOON | 2066 z Overhead +6.99% +13.02% | +14.95% | 42.25% | TimeMiver | 1304 | 749 | 20,002 | TOTS % | Crossformer 17,355 79.031 82.472 61.822, MiP | TimeKAN | 1456 | <550~—~«(~SCSO)—=«dY~C*«i 4SRS 18,978 36.674 90.268 62.174 [SRSNat| 1012 | 227 | a0 | S189 Overhead 49.35% 49.67% 49.45% | 40.57%\n",
      "Images: ['data/images/figure8.png', 'data/images/figure9.png', 'data/images/figure12.png']\n",
      "Fusion alpha: 0.5\n"
     ]
    }
   ],
   "source": [
    "def _normalize_scores(pairs):\n",
    "    \"\"\"Min-max normalize a list of (idx, score) to [0,1].\n",
    "    If all scores equal, returns 1.0 for each item (so ordering stays stable).\n",
    "    \"\"\"\n",
    "    if not pairs:\n",
    "        return []\n",
    "    scores = [s for _, s in pairs]\n",
    "    lo, hi = min(scores), max(scores)\n",
    "    if abs(hi - lo) < 1e-12:\n",
    "        return [(i, 1.0) for i, _ in pairs]\n",
    "    return [(i, (s - lo) / (hi - lo)) for i, s in pairs]\n",
    "\n",
    "\n",
    "def build_context(\n",
    "    question: str,\n",
    "    top_k_text: int = TOP_K_TEXT,\n",
    "    top_k_images: int = TOP_K_IMAGES,\n",
    "    top_k_evidence: int = TOP_K_EVIDENCE,\n",
    "    alpha: float = ALPHA,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Build a multimodal context block for the question.\n",
    "\n",
    "    Students:\n",
    "    - `top_k_text` / `top_k_images` control *candidate retrieval* per modality.\n",
    "    - `top_k_evidence` controls the *final context size*.\n",
    "    - `alpha` controls fusion: higher = prefer text evidence, lower = prefer images.\n",
    "\n",
    "    This function returns:\n",
    "    - `context`: a text block with the selected evidence (what you pass to an LLM)\n",
    "    - `image_paths`: paths of images selected as evidence\n",
    "    - `evidence`: structured evidence list (recommended for your report)\n",
    "    \"\"\"\n",
    "    # 1) Retrieve candidates from each modality\n",
    "    text_hits = tfidf_retrieve(question, text_vec, text_X, top_k=top_k_text)   # [(idx, score), ...]\n",
    "    img_hits  = tfidf_retrieve(question, img_vec,  img_X,  top_k=top_k_images)\n",
    "\n",
    "    # 2) Normalize scores per modality and fuse with ALPHA\n",
    "    text_norm = _normalize_scores(text_hits)\n",
    "    img_norm  = _normalize_scores(img_hits)\n",
    "\n",
    "    fused = []\n",
    "    for idx, s in text_norm:\n",
    "        ch = page_chunks[idx]\n",
    "        fused.append({\n",
    "            \"modality\": \"text\",\n",
    "            \"id\": ch.chunk_id,\n",
    "            \"raw_score\": float(dict(text_hits).get(idx, 0.0)),\n",
    "            \"fused_score\": float(alpha * s),\n",
    "            \"text\": ch.text,\n",
    "            \"path\": None,\n",
    "        })\n",
    "\n",
    "    for idx, s in img_norm:\n",
    "        it = image_items[idx]\n",
    "        fused.append({\n",
    "            \"modality\": \"image\",\n",
    "            \"id\": it.item_id,\n",
    "            \"raw_score\": float(dict(img_hits).get(idx, 0.0)),\n",
    "            \"fused_score\": float((1.0 - alpha) * s),\n",
    "            \"text\": it.caption,     # we retrieve on caption/filename text\n",
    "            \"path\": it.path,\n",
    "        })\n",
    "\n",
    "    # 3) Pick top fused evidence\n",
    "    fused = sorted(fused, key=lambda d: d[\"fused_score\"], reverse=True)[:top_k_evidence]\n",
    "\n",
    "    # 4) Build the context string (what you feed into a generator/LLM)\n",
    "    ctx_lines = []\n",
    "    image_paths = []\n",
    "    for ev in fused:\n",
    "        if ev[\"modality\"] == \"text\":\n",
    "            snippet = (ev[\"text\"] or \"\")[:260].replace(\"\\n\", \" \")\n",
    "            ctx_lines.append(f\"[TEXT | {ev['id']} | fused={ev['fused_score']:.3f}] {snippet}\")\n",
    "        else:\n",
    "            ctx_lines.append(f\"[IMAGE | {ev['id']} | fused={ev['fused_score']:.3f}] caption={ev['text']}\")\n",
    "            image_paths.append(ev[\"path\"])\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"context\": \"\\n\".join(ctx_lines),\n",
    "        \"image_paths\": image_paths,\n",
    "        \"text_hits\": text_hits,\n",
    "        \"img_hits\": img_hits,\n",
    "        \"evidence\": fused,\n",
    "        \"alpha\": alpha,\n",
    "        \"top_k_text\": top_k_text,\n",
    "        \"top_k_images\": top_k_images,\n",
    "        \"top_k_evidence\": top_k_evidence,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Demo: what retrieval returns for one query ---\n",
    "ctx_demo = build_context(mini_gold[0][\"question\"])\n",
    "print(ctx_demo[\"context\"])\n",
    "print(\"Images:\", ctx_demo[\"image_paths\"])\n",
    "print(\"Fusion alpha:\", ctx_demo[\"alpha\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cetviWwW7G2R",
   "metadata": {
    "id": "cetviWwW7G2R"
   },
   "source": [
    "## Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "Rt3DrR8k7JjE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449,
     "referenced_widgets": [
      "197f21a391cf4ab9841a0393c043bc79",
      "9f46233623554a22bb717e6ebba43699",
      "5a333afda36c4af4a5f0331c727fc236",
      "096518bfdabe428d89006216ab6ec1f4",
      "64fb2bdb25e34993a5e3f02261d6ccee",
      "0d803f5d68db416faee6bbe5b326e205",
      "312218c3e9834f57aa5683a3d1626bb6",
      "92ae02f49ccf4d309dec44b3665d22a7",
      "f2a8cbcc685348fb8fbc81f01f4875a1",
      "1fa0e57efba8446783fec40c5d1a9c5f",
      "de16f718c2bd4c9ab33fa0e0fd8c1414",
      "d8ef82502fef4adfa03aadc32af066b9",
      "8d092a37ada3497e933b89eaf73d54ef",
      "2ceafdd82f974baea069b6e906275fea",
      "159302077f7349bc9bc9168a89a7e9a6",
      "db1d03d729c84e1c8c6384e4fc6868de",
      "098b3cdbed114604a041f241a053a323",
      "3d22641270d84a8ea8e7ead494e2e70d",
      "ceef58e96a8449508923f9ec68f6111a",
      "ccbdc724c3b34a04b6458834cfa18cf2",
      "8584f1a85e9749c0bd0ac17dd7b8ef54",
      "2ab78738e5da466a838cd4406e35c445",
      "050e6fd54a5640e799a31d211d985b34",
      "649d1720ae054beba73a1defc0f9b7d2",
      "ceadc06c44e049e895d84f714d22b05c",
      "f01359eba517480b9706cfaeb59eb478",
      "8d4ecb1ea67b43999419c6c7ed2ab049",
      "a09a879cb22c401cb1186c46f6fe27f0",
      "084b0a08cac946eb9e7611ed1ef6d12a",
      "41fd4722c3a44fa9b01b895ad28a3d96",
      "691f86453b874661904c9ac58a5fba43",
      "e181bbea7933434fb373ef72030c05dc",
      "c01e78841b994fc48b7fefc2dd878076",
      "4abc8ecd80104d839da85dc80f817255",
      "1985d8c03a964e46869e7809ca127b84",
      "5cbb8f41195141c39fa7892d11ae61ac",
      "bd8449ceb4494031b3e0fad08ec016fc",
      "6af58b6772d44cc1ba1672fe801921a6",
      "a74d641676c2491183dd0941cd1f32cd",
      "38bf2b06b5ee4993b80c49ea18c44c40",
      "db9ce17dee4f48a9b5cfaa32de0ca249",
      "ca931740ab234bb4bdccfafce44eac82",
      "fd06fccb9a5544319c7167454df68828",
      "e2ef5b1552124551ad2d64330045243d",
      "77da36675633461eb462a62daf4a9412",
      "f040364257be4d2cbdb2ac185e18c5b0",
      "2d0e0d67400042bfbba7998d2ba57e65",
      "6c42f8012fc747edbca58f930ac82897",
      "e540286e01a246ae83fcd2ccabfc4fe0",
      "8470fe87176e4079850d434423459a4e",
      "fab36cb6feda47c6bdc096b18532dcac",
      "aa156280a9a24122989bb77b2d9a9ce9",
      "1fd9f7906c3d45d7935d6a438d88818b",
      "04b2da524e6a44f8b4b40222f41a1300",
      "839629ecfcdd493fb09e7967693bcd60",
      "a63e8fbfe25e4b0e85c369d5ed8db057",
      "da90082d754940198b9f81fb2a02528d",
      "bb067d91efbe42b5a780aaec2f728699",
      "aae21ad3d7734a9ba6d5c85c00ad3c80",
      "eca0454111d648c985b7936663227c8c",
      "3c24088b421b4b85959fae009542bd7e",
      "94aa4ded30ed45408df1b40e2f1379c1",
      "24f3d4c2c52e4965acb0f43254eec19e",
      "30729ebbe8b74577a82c0fceaeb71d60",
      "06f30497685546eb8bb591024b09c722",
      "20a419643b684a3b9c7b9e31abdb1689",
      "1e0d9df18ed64f1b8fadfffe973601f1",
      "688460b61c0a45d3aecbe6e74dfaef15",
      "d6c99e90beac45d7af7667654347a646",
      "2690efbf4bc44346b07914f2df0e62a0",
      "97305a3d0e6c43c78cc74aaa9b2709e1",
      "54803fc0297a45d9a7ae91a17a210852",
      "fdd4999c409e4987ba08a1cc10aa4a95",
      "5e9369dea0d448e1a5cea735cca24ec7",
      "ea6de313f119466fb46554c1eca74470",
      "e6b909efc0a04ad981196c4a9fe83231",
      "9e18078604cf49369467c6188b5f39bc",
      "62a3399450034e75b713405a6ae4e697",
      "7fea5faa5e24489c9c3c6f990736715f",
      "d53fbbe084e9498ab900cddd1b9c1e47",
      "64836da6aa15428d8a753a084c87c320",
      "09b40b981bf34e16b9ba5211dea58c49",
      "b3f52faf435b4a1bb292f68b4907585a",
      "3ff377f4791d4c3f8ae03fedf11e0ace",
      "34482fe5571d4056a3b887872b29cf06",
      "43623cf5fe004012b13286deee9f39d7",
      "2539c6b9ffba4636ad1e35aedcc09428",
      "642a40490e9a46f98fa2df87fd916550"
     ]
    },
    "id": "Rt3DrR8k7JjE",
    "outputId": "9a6356a4-eb79-4370-ab21-bd9f509f50cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Reranker...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197f21a391cf4ab9841a0393c043bc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ef82502fef4adfa03aadc32af066b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050e6fd54a5640e799a31d211d985b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abc8ecd80104d839da85dc80f817255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77da36675633461eb462a62daf4a9412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63e8fbfe25e4b0e85c369d5ed8db057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0d9df18ed64f1b8fadfffe973601f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a3399450034e75b713405a6ae4e697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reranker loaded.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load a standard reranking model (trained on MS MARCO)\n",
    "# This model outputs a score (higher is better, usually unbounded but often -10 to 10)\n",
    "print(\"Loading Reranker...\")\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "print(\"✅ Reranker loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "59CxsRCP7RCS",
   "metadata": {
    "id": "59CxsRCP7RCS"
   },
   "outputs": [],
   "source": [
    "def normalize_scores(hits):\n",
    "    \"\"\"Normalizes a list of (idx, score) to 0..1 range.\"\"\"\n",
    "    if not hits: return []\n",
    "    scores = [s for _, s in hits]\n",
    "    min_s, max_s = min(scores), max(scores)\n",
    "    if max_s == min_s: return [(i, 1.0) for i, _ in hits]\n",
    "    return [(i, (s - min_s) / (max_s - min_s)) for i, s in hits]\n",
    "\n",
    "def get_retrieval_results(query: str, method: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieves candidate chunks based on the specified method.\n",
    "    Returns a list of (chunk_index, score).\n",
    "    \"\"\"\n",
    "    # 1. SPARSE ONLY\n",
    "    if method == \"Sparse Only\":\n",
    "        return tfidf_retrieve(query, text_vec, text_X, top_k=top_k)\n",
    "\n",
    "    # 2. DENSE ONLY\n",
    "    if method == \"Dense Only\":\n",
    "        # Assumes dense_retrieve exists from previous step\n",
    "        return dense_retrieve(query, top_k=top_k)\n",
    "\n",
    "    # 3. HYBRID (Sparse + Dense)\n",
    "    if method == \"Hybrid\" or method == \"Hybrid + Rerank\" or method == \"Multimodal\":\n",
    "        # Retrieve more candidates (e.g., top_k * 2) from both to ensure overlap\n",
    "        sparse_hits = tfidf_retrieve(query, text_vec, text_X, top_k=top_k*2)\n",
    "        dense_hits = dense_retrieve(query, top_k=top_k*2)\n",
    "\n",
    "        # Create a dict to fuse scores: {idx: fused_score}\n",
    "        fusion_map = {}\n",
    "\n",
    "        # Normalize and weigh (Alpha=0.5 usually works well for Hybrid)\n",
    "        for idx, score in normalize_scores(sparse_hits):\n",
    "            fusion_map[idx] = fusion_map.get(idx, 0) + (0.5 * score)\n",
    "\n",
    "        for idx, score in normalize_scores(dense_hits):\n",
    "            fusion_map[idx] = fusion_map.get(idx, 0) + (0.5 * score)\n",
    "\n",
    "        # Sort by fused score\n",
    "        hybrid_results = sorted(fusion_map.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # If just Hybrid, return top_k\n",
    "        if method == \"Hybrid\":\n",
    "            return hybrid_results[:top_k]\n",
    "\n",
    "        # 4. RERANKING (Re-score the hybrid candidates)\n",
    "        # We take the top 20 hybrid candidates and rerank them\n",
    "        candidates = hybrid_results[:20]\n",
    "\n",
    "        # Prepare pairs for CrossEncoder: [[query, doc_text], ...]\n",
    "        pairs = []\n",
    "        for idx, _ in candidates:\n",
    "            pairs.append([query, page_chunks[idx].text])\n",
    "\n",
    "        # Predict scores\n",
    "        rerank_scores = reranker.predict(pairs)\n",
    "\n",
    "        # Attach new scores to indices\n",
    "        reranked_results = []\n",
    "        for i, (idx, _) in enumerate(candidates):\n",
    "            reranked_results.append((idx, float(rerank_scores[i])))\n",
    "\n",
    "        # Sort by new reranker score\n",
    "        final_ranked = sorted(reranked_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return final_ranked[:top_k]\n",
    "\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DX_xbawXm-M1",
   "metadata": {
    "id": "DX_xbawXm-M1"
   },
   "source": [
    "## “Generator” (simple, offline)\n",
    "To keep this notebook runnable anywhere, we implement a **lightweight extractive generator**:\n",
    "- It returns the top evidence lines\n",
    "- In addition , we implement LLM Call with HF local model\n",
    "- LLM call with API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "vrswbIiMnoDt",
   "metadata": {
    "id": "vrswbIiMnoDt"
   },
   "outputs": [],
   "source": [
    "# Fusion knob (text vs images)\n",
    "ALPHA = 0.5  # 0.0 = images dominate, 1.0 = text dominates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6iRVkUNncC6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6iRVkUNncC6",
    "outputId": "37f2609b-4130-4879-f363-1950a4b384e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q1 What is the Selective Representation Space (SRS) module in SRSNet and how does it differ from conventional adjacent patching?\n",
      "Question: What is the Selective Representation Space (SRS) module in SRSNet and how does it differ from conventional adjacent patching?\n",
      "\n",
      "Grounded answer (extractive):\n",
      "[TEXT | doc1_TimeSeries.pdf::p2 | fused=0.500] Contextual Series Forecasting Horizon Loss of Information Out of distribution Disturbance from noise Changeable Periods Shifting Anomalies Adjacent Patching Selective Patching Changeable Periods Shifting Anomalies Retainment of periodicity In distribution Avoi\n",
      "[IMAGE | figure8.png | fu\n",
      "Images: ['figure8.png', 'figure9.png', 'figure12.png']\n",
      "\n",
      "================================================================================\n",
      "Q2 How does ReMindRAG's memory replay mechanism improve retrieval for similar or repeated queries?\n",
      "Question: How does ReMindRAG's memory replay mechanism improve retrieval for similar or repeated queries?\n",
      "\n",
      "Grounded answer (extractive):\n",
      "[TEXT | doc2_ReMindRAG.pdf::p8 | fused=0.500] subsequently re-evaluated again on the same dataset. (2) Similar Query: The model is re-evaluated again on a dataset A′ whose queries are semantically equivalent paraphrases of those in A (cf. Appendix C.3 for implementation). (3) Different Query: The model is\n",
      "[IMAGE | figure3.png | fused=0.500] caption=Caption: fig\n",
      "Images: ['figure3.png', 'figure12.png', 'figure7.png']\n",
      "\n",
      "================================================================================\n",
      "Q3 What real-world applications of the Consensus Planning Problem (CPP) are described, and what agent interfaces does each application use?\n",
      "Question: What real-world applications of the Consensus Planning Problem (CPP) are described, and what agent interfaces does each application use?\n",
      "\n",
      "Grounded answer (extractive):\n",
      "[TEXT | doc3_CPP.pdf::p3 | fused=0.500] next section the three main types of agents considered in our consensus planning problem: primal, dual, and proximal agents. 2.2 Agents In our consensus planning problem, there exist three types of agent interfaces, primal, dual, and proximal, which are the na\n",
      "[IMAGE | figure2.png \n",
      "Images: ['figure2.png', 'figure1.png', 'figure11.png']\n",
      "\n",
      "================================================================================\n",
      "Q4 According to Table 1 in the ReMindRAG paper, what is the Multi-Hop QA accuracy of ReMindRAG with the Deepseek-V3 backbone, and how does it compare to HippoRAG2 on the same task and backbone?\n",
      "Question: According to Table 1 in the ReMindRAG paper, what is the Multi-Hop QA accuracy of ReMindRAG with the Deepseek-V3 backbone, and how does it compare to HippoRAG2 on the same task and backbone?\n",
      "\n",
      "Grounded answer (extractive):\n",
      "[TEXT | doc2_ReMindRAG.pdf::p7 | fused=0.500] 2) Multi-Hop QA: We resort to the HotpotQA dataset [49]. It focuses on complex question- answering scenarios requiring multi-step reasoning, demanding the model to chain scattered information through logical inference. 3) \n",
      "Images: ['figure6.png', 'figure5.png', 'figure3.png']\n",
      "\n",
      "================================================================================\n",
      "Q5 What reinforcement learning reward function does SRSNet use to train the Selective Patching scorer?\n",
      "Question: What reinforcement learning reward function does SRSNet use to train the Selective Patching scorer?\n",
      "\n",
      "Grounded answer (extractive):\n",
      "[TEXT | doc1_TimeSeries.pdf::p2 | fused=0.500] Contextual Series Forecasting Horizon Loss of Information Out of distribution Disturbance from noise Changeable Periods Shifting Anomalies Adjacent Patching Selective Patching Changeable Periods Shifting Anomalies Retainment of periodicity In distribution Avoi\n",
      "[IMAGE | figure8.png | fused=0.500] caption=Caption\n",
      "Images: ['figure8.png', 'figure9.png', 'figure14.png']\n",
      "\n",
      "================================================================================\n",
      "Q6 What are the key stages shown in the ReMindRAG overall workflow diagram?\n",
      "Question: What are the key stages shown in the ReMindRAG overall workflow diagram?\n",
      "\n",
      "Grounded answer (extractive):\n",
      "[TEXT | doc2_ReMindRAG.pdf::p2 | fused=0.500] the nuanced semantic relationships embedded within the graph, leading to unsatisfactory system effectiveness [13, 10, 18]. By contrast, the LLM-guided knowledge graph traversal approaches demonstrate notable advantages [6, 42, 41], but these methods lead to su\n",
      "[IMAGE | figure3.png | fused=0.500] caption=Caption: figure3. Content: First Qu\n",
      "Images: ['figure3.png', 'figure5.png', 'figure8.png']\n"
     ]
    }
   ],
   "source": [
    "#Method 1: Lightweight extractive generator\n",
    "\n",
    "def simple_extractive_answer(question: str, context: str) -> str:\n",
    "    lines = context.splitlines()\n",
    "    if not lines:\n",
    "        return \"I don't know (no evidence retrieved).\"\n",
    "    # Return top 2 evidence lines as a \"grounded\" answer\n",
    "    return (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        \"Grounded answer (extractive):\\n\"\n",
    "        + \"\\n\".join(lines[:2])\n",
    "    )\n",
    "\n",
    "def run_query(qobj, top_k_text=TOP_K_TEXT, top_k_images=TOP_K_IMAGES, top_k_evidence=TOP_K_EVIDENCE, alpha=ALPHA) -> Dict[str, Any]:\n",
    "    question = qobj[\"question\"]\n",
    "    ctx = build_context(question, top_k_text=top_k_text, top_k_images=top_k_images, top_k_evidence=top_k_evidence, alpha=alpha)\n",
    "    answer = simple_extractive_answer(question, ctx[\"context\"])\n",
    "    return {\n",
    "        \"id\": qobj[\"query_id\"], # Fixed: changed from \"id\" to \"query_id\"\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"context\": ctx[\"context\"],\n",
    "        \"image_paths\": ctx[\"image_paths\"],\n",
    "        \"text_hits\": ctx[\"text_hits\"],\n",
    "        \"img_hits\": ctx[\"img_hits\"],\n",
    "    }\n",
    "\n",
    "results = [run_query(q) for q in mini_gold]\n",
    "for r in results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(r[\"id\"], r[\"question\"])\n",
    "    print(r[\"answer\"][:500])\n",
    "    print(\"Images:\", [os.path.basename(p) for p in r[\"image_paths\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bJ56IsdLpId2",
   "metadata": {
    "id": "bJ56IsdLpId2"
   },
   "source": [
    "## Generator using LLM (API Call) with model gemini-2.5-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "WECqdUHGpJU7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "id": "WECqdUHGpJU7",
    "outputId": "feb935da-e524-49a0-899d-121526b93241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[Q1] Question: What is the Selective Representation Space (SRS) module in SRSNet and how does it differ from conventional adjacent patching?\n",
      "--------------------------------------------------------------------------------\n",
      "LLM Answer:\n",
      "The Selective Representation Space (SRS) module in SRSNet consists of Selective Patching, Dynamic Reassembly, and Adaptive Fusion [IMAGE | figure8.png]. Selective Patching adaptively chooses patches from all potential candidates by scanning them with a stride of 1, generating scores for each, and then retrieving patches with the maximum scores in each sampling. It allows sampling with replacement [IMAGE | figure9.png]. Dynamic Reassembly subsequently generates scores for these selected patches and sorts them to determine their sequence [IMAGE | figure9.png]. Finally, Adaptive Fusion integrates the embeddings from Dynamic Reassembly and Conventional Patching, adding position embeddings to form the final representations [IMAGE | figure8.png].\n",
      "\n",
      "The key difference from conventional adjacent patching is that adjacent patching typically involves contiguous, non-adaptive patching [IMAGE | figure8.png], whereas Selective Patching within the SRS module *adaptively chooses* patches based on scores and allows for non-contiguous selection and dynamic reordering [IMAGE | figure8.png][IMAGE | figure9.png].\n",
      "--------------------------------------------------------------------------------\n",
      "Context Images: ['figure8.png', 'figure9.png', 'figure12.png']\n",
      "\n",
      "================================================================================\n",
      "[Q2] Question: How does ReMindRAG's memory replay mechanism improve retrieval for similar or repeated queries?\n",
      "--------------------------------------------------------------------------------\n",
      "LLM Answer:\n",
      "ReMindRAG improves retrieval for similar or repeated queries by memorizing LLM-guided traversal paths, enabling fast retrieval when similar queries are encountered subsequently [IMAGE | figure1.png]. In cases of memory replay for the \"Same Query\" setting, after an initial search, the system enhances useful paths and penalizes irrelevant ones. This allows a subsequent search to focus on the subgraph with the correct information, achieving self-correction [IMAGE | figure7.png].\n",
      "--------------------------------------------------------------------------------\n",
      "Context Images: ['figure3.png', 'figure12.png', 'figure7.png']\n",
      "\n",
      "================================================================================\n",
      "[Q3] Question: What real-world applications of the Consensus Planning Problem (CPP) are described, and what agent interfaces does each application use?\n",
      "--------------------------------------------------------------------------------\n",
      "LLM Answer:\n",
      "Real-world applications of the Consensus Planning Problem (CPP) and their agent interfaces include:\n",
      "\n",
      "*   **Fullness Optimization**: Coordinates inventory buying with physical network capacity. It uses a dual interface for the buying agent and a primal interface for the capacity agent [IMAGE | figure2.png][TEXT | doc3_CPP.pdf::p4].\n",
      "*   **Throughput coordination**: Coordinates inventory flows throughout different regions (e.g., inbound, transfer, outbound) given network labor constraints. All agents use a proximal interface [IMAGE | figure2.png].\n",
      "*   **Transportation optimization**: Coordinates transportation capacity across delivery stations and third-party carriers. All agents use a proximal interface [IMAGE | figure2.png].\n",
      "*   **Arrivals and throughput coordination**: Coordinates inbound inventory flows in a single region. The buying agent has a dual interface, while the throughput agent may use a primal or dual interface [IMAGE | figure2.png].\n",
      "--------------------------------------------------------------------------------\n",
      "Context Images: ['figure2.png', 'figure1.png', 'figure11.png']\n",
      "\n",
      "================================================================================\n",
      "[Q4] Question: According to Table 1 in the ReMindRAG paper, what is the Multi-Hop QA accuracy of ReMindRAG with the Deepseek-V3 backbone, and how does it compare to HippoRAG2 on the same task and backbone?\n",
      "--------------------------------------------------------------------------------\n",
      "LLM Answer:\n",
      "According to Table 1, ReMindRAG (Ours) achieves a Multi-Hop QA accuracy of 79.38% with the Deepseek-V3 backbone. This is higher than HippoRAG2, which has an accuracy of 64.95% for the same task and backbone [IMAGE | figure6.png].\n",
      "--------------------------------------------------------------------------------\n",
      "Context Images: ['figure6.png', 'figure5.png', 'figure3.png']\n",
      "\n",
      "================================================================================\n",
      "[Q5] Question: What reinforcement learning reward function does SRSNet use to train the Selective Patching scorer?\n",
      "--------------------------------------------------------------------------------\n",
      "LLM Answer:\n",
      "Not enough evidence in the retrieved context. The context states that Selective Patching is \"gradient-based and learnable\" [IMAGE | figure8.png] and describes how it generates scores [IMAGE | figure9.png], but it does not specify a reinforcement learning reward function for training its scorer.\n",
      "--------------------------------------------------------------------------------\n",
      "Context Images: ['figure8.png', 'figure9.png', 'figure14.png']\n",
      "\n",
      "================================================================================\n",
      "[Q6] Question: What are the key stages shown in the ReMindRAG overall workflow diagram?\n",
      "--------------------------------------------------------------------------------\n",
      "LLM Answer:\n",
      "The overall workflow of ReMindRAG includes constructing a Knowledge Graph (KG) from unstructured text, memorizing LLM-guided traversal paths, and then, for a given query, it goes through stages like \"Get Seed Node,\" \"Get Ans Subgraph,\" \"Traversal,\" and \"Memorize,\" enabling fast retrieval for similar subsequent queries [IMAGE | figure3.png].\n",
      "--------------------------------------------------------------------------------\n",
      "Context Images: ['figure3.png', 'figure5.png', 'figure8.png']\n"
     ]
    }
   ],
   "source": [
    "# Method 2: LLM extractive generator (API Call)\n",
    "\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# --- SETUP LLM ---\n",
    "# Set up secret key on the left side bar\n",
    "try:\n",
    "    api_key = userdata.get('GEMINI_API_KEY')\n",
    "except Exception:\n",
    "    api_key = \"PASTE_YOUR_KEY_HERE\"\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = api_key\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "def generate_llm_answer(question: str, context: str) -> str:\n",
    "    \"\"\"Generates an answer using an LLM (Gemini) based on the provided context.\"\"\"\n",
    "\n",
    "    # 1. Check for empty context\n",
    "    if not context or not context.strip():\n",
    "        return \"Not enough evidence in the retrieved context.\"\n",
    "\n",
    "    # 2. Define the model\n",
    "    # Using gemini-2.5-flash as it is widely available and free-tier friendly\n",
    "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "    # 3. Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant for a Multimodal RAG system.\n",
    "    Use the following retrieved context (text chunks and image descriptions) to answer the user's question.\n",
    "\n",
    "    RULES:\n",
    "    1. Answer ONLY using the provided context. If the answer is not in the context, say \"Not enough evidence in the retrieved context.\"\n",
    "    2. Cite your sources! When you use information, append the source ID like [TEXT | doc1.pdf::p1] or [IMAGE | figure1.png].\n",
    "    3. Be concise and direct.\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    QUESTION:\n",
    "    {question}\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    # 4. Call the API\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"LLM Generation Error: {str(e)} (Check your API Key)\"\n",
    "\n",
    "# --- UPDATED RUN_QUERY ---\n",
    "def run_query(qobj, top_k_text=TOP_K_TEXT, top_k_images=TOP_K_IMAGES, top_k_evidence=TOP_K_EVIDENCE, alpha=ALPHA) -> Dict[str, Any]:\n",
    "    question = qobj[\"question\"]\n",
    "\n",
    "    # 1. Retrieve and Build Context\n",
    "    ctx = build_context(question, top_k_text=top_k_text, top_k_images=top_k_images, top_k_evidence=top_k_evidence, alpha=alpha)\n",
    "\n",
    "    # 2. Generate Answer with LLM (Replaces simple_extractive_answer)\n",
    "    answer = generate_llm_answer(question, ctx[\"context\"])\n",
    "\n",
    "    return {\n",
    "        \"id\": qobj[\"query_id\"],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"context\": ctx[\"context\"],\n",
    "        \"image_paths\": ctx[\"image_paths\"],\n",
    "        \"text_hits\": ctx[\"text_hits\"],\n",
    "        \"img_hits\": ctx[\"img_hits\"],\n",
    "    }\n",
    "\n",
    "# --- EXECUTION ---\n",
    "results = [run_query(q) for q in mini_gold]\n",
    "\n",
    "for r in results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"[{r['id']}] Question: {r['question']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"LLM Answer:\\n{r['answer']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Context Images:\", [os.path.basename(p) for p in r[\"image_paths\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crolYJYkpyLB",
   "metadata": {
    "id": "crolYJYkpyLB"
   },
   "source": [
    "## Generator using HuggingFace LLM (local) with flan-t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "n2gdliERpzWV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2gdliERpzWV",
    "outputId": "64981510-727d-4e74-f164-bb055e54cbeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install -q transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "WoPoobaEp3fx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325,
     "referenced_widgets": [
      "b6c806bc69154792a96a9b402c6cde7b",
      "d12c75daa20f439987fa40636403cc7f",
      "9dbe8ad661464dd28eedd801613114d5",
      "c756945e94044a1fa35ff21415f75d1c",
      "9e5a3393fad9485cb312ef538e80efc6",
      "7624cd356d5a49999008864f7f4b60b0",
      "0a4e10278cb94202bf04ed812a3f72e4",
      "ae1f76e1ddb1400abc66e20afcba430a",
      "a7d72c5a57904f149e40b796eda33eea",
      "ea7dcd4ec36941a1896d03f1e39e2be4",
      "b1541000a40e48c1ad72385cd38bbb83",
      "d5b625480947402cbe89d3ada42bf327",
      "8b415df6e1964d7ca22a6a28af4be8a6",
      "6bf997eaff984577a6155e3c9999d204",
      "e3475e484eb14dd08d8c37c03946f01d",
      "ffba4e8c38264281b85c9c00bdf67028",
      "fe87e15e928b4b0e95bca7ebba64777a",
      "22cce8de555a4a8f93b73d0b9c29d51e",
      "8333214f9cda40e0ae283acaf1415788",
      "5d46735b44374766a676313eb28cc628",
      "58042d4984c248bdb78be24fb76ea17c",
      "d24e710f84c54332b76042c4949dca05",
      "7c68ef75e19c4a2ba4e11cb5beb35668",
      "64f82e5d6e18488e81e758de4c6b1319",
      "46a9cbfabdeb4e819d809819e0691119",
      "b76ca792c9694133b1dfadc645008bc2",
      "b9c4644497a4482abfb6de302ce91767",
      "2ea680b766e44bb68f4ea4a7a72df8f9",
      "566a0f1aa0684984aad2017053d5ec93",
      "c5b09341a7a14cf7870ec50a31780138",
      "ab1bc454f3e549f2ba8a72e6292a4f6c",
      "a6116ddd9a5c486c8bdfaa7c86f69a75",
      "3edb9477bb5f4f848f0cb43677a4fb6b",
      "1e45af78839c4edd9eed885909b33390",
      "580989dee4894573ab94ea2c54a9288d",
      "92b03fcb6b994fe292085a66fc0387f5",
      "7a746c1ede854ff8a6eeb36614630753",
      "c340a4e12fad4b548046b7896e2dd89b",
      "3eba7d188f034ba6b12eb9aab01213f7",
      "828edf0cef284860a981e96166cb18fa",
      "ef4959f6c0a24fc38349d635d4f3f169",
      "8c876086b410402d9f918c762eb061b8",
      "6b95eeab8c3b42899dc548bf0bbcea3c",
      "0989a98ebb8f461d88715ab464e2cb3d",
      "d93a2444355449f3b2ff286a0cd92c45",
      "25655b912670498e98d9d8bb3281a371",
      "ed2062cd566542659ab93d0d9278c33b",
      "a2c5e300f18b4a0fa8b5a3072b595647",
      "353fc6d76e964351a142c88618d65fc2",
      "66c70eaf2416442fb4769c45fbdb0259",
      "065d29ddad6a437abfdda4c51d7724cf",
      "11d9f514bd194ded8550b38ecbcd2e6a",
      "d26c4da26f3942a2bea90de89a93eb56",
      "79d26658fc9445c19f210e186305c0c4",
      "ab276654395742489b53a0bc2e39da6f",
      "7bad47c8599641f4b90d2ca9ab6e1e65",
      "ef114dcebd594acca1d5f911e0c96862",
      "698c7a14e8ec4e5b9c83dfc48897f3a8",
      "14eae827940e4bdea2c92784b7085689",
      "a5fb55e757b64a27abc10bcca91bfc76",
      "6a07b80362754cc7a01f680cdb2f7dd5",
      "4b2df702ecff47f48963730db7ef4174",
      "8c85ff2001d64225891fcebeddf229ab",
      "55b37dfe5e1745f9b223125bedfc5320",
      "67df5de1288d42b7887e7e734c728ca1",
      "970683c615a74298b0304889e8a139d4",
      "ad745af52fac4679a014cb5e123b2017",
      "f35f4cd3df5c4351be9c8659bc541d96",
      "efbad4705e56436180632aa155b88bad",
      "5541ed9b7977450b91918a92f9358e0e",
      "bd8d32c76e774f0c81ea2f59009603a3",
      "423b96aca3d04e6eafa9c067c8757c43",
      "22b9f48127394100b1efb63a2ea780e5",
      "6a024a428e58474ebef6e0c855ad8761",
      "e87a76d48fb74016b38c3783df9fa6c0",
      "2fcdf5af34044ae59a581dc2031ca334",
      "5e56b6c3868144ed858e1944cf89016e",
      "e8a2f3fb345b4258ac042dd21286ce54",
      "1c31de312b4d472aa4c4eed60c15659a",
      "0e5f965933074ade8975ca42065982cc",
      "28a0d720d5314b0abea3e85874b5d8d2",
      "a8be37cd72cb4b86b612910b9704e71e",
      "f76abddfb96146489566a3ddf6c19cac",
      "e8883c5c432c40b8aa2d72584b6c056a",
      "5c89b65685624370ba20eb00c9f36262",
      "1a9914f7d31c4a3fb73825248103334c",
      "2365101872fb4ec3a4d58aa413fe6c5e",
      "2fd7b8882fc44e5bba61a3907c22ab62"
     ]
    },
    "id": "WoPoobaEp3fx",
    "outputId": "b206fa31-19d7-4705-b30c-7c2956f41c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c806bc69154792a96a9b402c6cde7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b625480947402cbe89d3ada42bf327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c68ef75e19c4a2ba4e11cb5beb35668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e45af78839c4edd9eed885909b33390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93a2444355449f3b2ff286a0cd92c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bad47c8599641f4b90d2ca9ab6e1e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad745af52fac4679a014cb5e123b2017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a2f3fb345b4258ac042dd21286ce54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Method 3: HuggingFace Local\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the local model (for extractive RAG)\n",
    "print(\"Loading local model...\")\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    # model=\"google/flan-t5-large\",\n",
    "    model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "print(\"✅ Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "VtOSfqQ9p8Xf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtOSfqQ9p8Xf",
    "outputId": "41e71914-b001-4d08-d332-f5bd8c6909f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=400) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local LLM queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=400) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=400) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=400) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=400) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=400) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q1 What is the Selective Representation Space (SRS) module in SRSNet and how does it differ from conventional adjacent patching?\n",
      "Question: What is the Selective Representation Space (SRS) module in SRSNet and how does it differ from conventional adjacent patching?\n",
      "\n",
      "LLM Answer:\n",
      "The Selective Representation Space (SRS) module in SRSNet utilizes selective patching to select suitable patches from all potential candidate patches to form the representation. This approach allows us to effectively capture the variability of the input data while maintaining the temporal coherence of the representation.\n",
      "Images: ['figure8.png', 'figure9.png', 'figure12.png']\n",
      "\n",
      "================================================================================\n",
      "Q2 How does ReMindRAG's memory replay mechanism improve retrieval for similar or repeated queries?\n",
      "Question: How does ReMindRAG's memory replay mechanism improve retrieval for similar or repeated queries?\n",
      "\n",
      "LLM Answer:\n",
      "ReMindRAG's memory replay mechanism enables fast retrieval when encountering similar or repeated queries subsequently by maintaining a KG on the fly. The memory replay mechanism employs a memory-efficient traversal path structure to guide the traversal towards the most relevant nodes, which can be reused for subsequent retrieval queries. This approach ensures that the KG can be queried efficiently with a consistent search experience and supports high retrieval efficiency for similar or repeated queries.\n",
      "Images: ['figure3.png', 'figure12.png', 'figure7.png']\n",
      "\n",
      "================================================================================\n",
      "Q3 What real-world applications of the Consensus Planning Problem (CPP) are described, and what agent interfaces does each application use?\n",
      "Question: What real-world applications of the Consensus Planning Problem (CPP) are described, and what agent interfaces does each application use?\n",
      "\n",
      "LLM Answer:\n",
      "The Consensus Planning Problem (CPP) is used in Amazon Supply Chain Optimization Technologies (ASCO) to coordinate inventory buying and transportation flows in several Amazon supply chain applications. The agents are buying agents and fullness (capacity) agents, and the interfaces are dual for the buying agent and proximal interface for the capacity agent. The throughput coordination objective is coordinate inventory flows, but this differs because there is only one region and only one inventory flow (inbound arrivals) is considered. The agents are each region, and the interfaces are proximal interface for the buying agent and throughput agent (analogous to throughput coordination problem). The arrivals and throughput coordination objective is like in throughput coordination, coordinate inventory flows, but this differs because there is only one region and only one inventory flow (inbound arrivals) is considered. The agents are each region, and the interfaces are dually interface for all agents. The primal, dual, and proximal agents interfaces differ for different configurations of the mix of agents and learning rates: pp = pa = px = 0.1 (top left), pp = pa = pr = 1 (top right), pp = pr = 10, pa = 1 (bottom left), pp = px = 50, pa = 1 (bottom right).\n",
      "Images: ['figure2.png', 'figure1.png', 'figure11.png']\n",
      "\n",
      "================================================================================\n",
      "Q4 According to Table 1 in the ReMindRAG paper, what is the Multi-Hop QA accuracy of ReMindRAG with the Deepseek-V3 backbone, and how does it compare to HippoRAG2 on the same task and backbone?\n",
      "Question: According to Table 1 in the ReMindRAG paper, what is the Multi-Hop QA accuracy of ReMindRAG with the Deepseek-V3 backbone, and how does it compare to HippoRAG2 on the same task and backbone?\n",
      "\n",
      "LLM Answer:\n",
      "According to Table 1 in the ReMindRAG paper, the Multi-Hop QA accuracy of ReMindRAG with the Deepseek-V3 backbone is 74.22%, which is significantly higher than HippoRAG2 with 58.51%.\n",
      "Images: ['figure6.png', 'figure5.png', 'figure3.png']\n",
      "\n",
      "================================================================================\n",
      "Q5 What reinforcement learning reward function does SRSNet use to train the Selective Patching scorer?\n",
      "Question: What reinforcement learning reward function does SRSNet use to train the Selective Patching scorer?\n",
      "\n",
      "LLM Answer:\n",
      "SRSNet uses the mean square error (MSE) as the reward function during training. The MSE is the average squared error between the predicted and the ground-truth time series scores.\n",
      "Images: ['figure8.png', 'figure9.png', 'figure14.png']\n",
      "\n",
      "================================================================================\n",
      "Q6 What are the key stages shown in the ReMindRAG overall workflow diagram?\n",
      "Question: What are the key stages shown in the ReMindRAG overall workflow diagram?\n",
      "\n",
      "LLM Answer:\n",
      "The key stages shown in the ReMindRAG overall workflow diagram are:\n",
      "\n",
      "1. Query aco ST => cot Ans Subgraph =——» Traversal Ce Seed Node Seni urec) reo Memorize Get Answer Subgraph Get Seed Node Get Ans Subgraph Meer ein\n",
      "2. Similarly Query: The model is re-evaluated again on a dataset A′ whose queries are semantically equivalent paraphrases of those in A (cf. Appendix C.3 for implementation). 3. Different Query: The model is re-evaluated again on a dataset A on which queries are semantically equivalent paraphrases of those in A (cf. Appendix C.3 for implementation). 4. Similar Query: The model is re-evaluated again on a dataset B on which queries are semantically equivalent paraphrases of those in B (cf. Appendix C.3 for implementation). 5. Conversely, Memorization is applied when encountering similar queries. 6. Similarly Re-evaluate on the same dataset. 7. Similar Query: The model is re-evaluated again on a dataset C on which queries are semantically equivalent paraphrases of those in C (cf. Appendix C.3 for implementation). 8. Different Query: The model is re-evaluated again on a dataset D on which queries are semantically equivalent paraphrases of those in D (cf. Appendix C.3 for implementation). 9. Similar Query: The model is re-evaluated again on a dataset E on which queries are semantically equivalent paraphrases of those in E (cf. Appendix C.3 for implementation). 10. Different Query: The model is re-evaluated again on a dataset F on which queries are semantically equivalent paraphrases\n",
      "Images: ['figure3.png', 'figure5.png', 'figure8.png']\n"
     ]
    }
   ],
   "source": [
    "def llm_extractive_answer(question: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces simple_extractive_answer with a local LLM generation.\n",
    "    \"\"\"\n",
    "    if not context or not context.strip():\n",
    "        return \"I don't know (no evidence retrieved).\"\n",
    "\n",
    "    tokenizer = llm_pipeline.tokenizer # Access tokenizer from the pipeline\n",
    "    max_context_tokens = 1500 # Safe limit (2048 total - 400 new - ~148 buffer)\n",
    "\n",
    "    # Tokenize the context\n",
    "    tokenized_context = tokenizer(context, truncation=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    # If context is too long, slice it\n",
    "    if tokenized_context.shape[1] > max_context_tokens:\n",
    "        # Keep the first 1500 tokens\n",
    "        tokenized_context = tokenized_context[:, :max_context_tokens]\n",
    "        # Decode back to string\n",
    "        context = tokenizer.decode(tokenized_context[0], skip_special_tokens=True)\n",
    "\n",
    "    # Prompt engineering\n",
    "    # Note: For TinyLlama, a simple format works, but we add \"Answer:\" to trigger the generation.\n",
    "    prompt = (\n",
    "        f\"Use the Context below to answer the Question. \"\n",
    "        f\"If the answer is not in the Context, say 'Not enough evidence in the retrieved context.'.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\"\n",
    "        f\"\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    # FIXED: Increased max_new_tokens to 400 (prevents cut-offs)\n",
    "    # FIXED: Set do_sample=True (prevents the \"1.1.1.1\" repetition loop)\n",
    "    output = llm_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=400,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        return_full_text=False\n",
    "    )\n",
    "    generated_text = output[0]['generated_text'].strip()\n",
    "\n",
    "    return (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"LLM Answer:\\n{generated_text}\"\n",
    "    )\n",
    "\n",
    "def run_query(qobj, top_k_text=TOP_K_TEXT, top_k_images=TOP_K_IMAGES, top_k_evidence=TOP_K_EVIDENCE, alpha=ALPHA) -> Dict[str, Any]:\n",
    "    question = qobj[\"question\"]\n",
    "\n",
    "    # 1. Build Context (Uses your existing function)\n",
    "    ctx = build_context(question, top_k_text=top_k_text, top_k_images=top_k_images, top_k_evidence=top_k_evidence, alpha=alpha)\n",
    "\n",
    "    # 2. Generate Answer\n",
    "    answer = llm_extractive_answer(question, ctx[\"context\"])\n",
    "\n",
    "    # 3. Return exact same structure as your original code\n",
    "    return {\n",
    "        \"id\": qobj[\"query_id\"],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"context\": ctx[\"context\"],\n",
    "        \"image_paths\": ctx[\"image_paths\"],\n",
    "        \"text_hits\": ctx[\"text_hits\"], # Preserved\n",
    "        \"img_hits\": ctx[\"img_hits\"],   # Preserved\n",
    "    }\n",
    "\n",
    "# --- EXECUTION ---\n",
    "print(\"Running local LLM queries...\")\n",
    "results = [run_query(q) for q in mini_gold]\n",
    "\n",
    "for r in results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(r[\"id\"], r[\"question\"])\n",
    "    print(r[\"answer\"])\n",
    "    print(\"Images:\", [os.path.basename(p) for p in r[\"image_paths\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OY4tuBMl7tyX",
   "metadata": {
    "id": "OY4tuBMl7tyX"
   },
   "source": [
    "## Retrieval Evaluation (Precision@k / Recall@k)\n",
    "We treat a text chunk as **relevant** for a query if it contains at least one `must_have_keywords` term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EakP-SMp70kR",
   "metadata": {
    "id": "EakP-SMp70kR"
   },
   "outputs": [],
   "source": [
    "def is_relevant_text(chunk_text: str, rubric: Dict[str, Any]) -> bool:\n",
    "    text = chunk_text.lower()\n",
    "    must = [k.lower() for k in rubric.get(\"must_have_keywords\", [])]\n",
    "    return any(k in text for k in must)\n",
    "\n",
    "def precision_at_k(relevances: List[bool], k: int) -> float:\n",
    "    k = min(k, len(relevances))\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    return sum(relevances[:k]) / k\n",
    "\n",
    "def recall_at_k(relevances: List[bool], k: int, total_relevant: int) -> float:\n",
    "    k = min(k, len(relevances))\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    return sum(relevances[:k]) / total_relevant\n",
    "\n",
    "def eval_retrieval_for_query(qobj, top_k=10) -> Dict[str, Any]:\n",
    "    question = qobj[\"question\"]\n",
    "    rubric = qobj[\"rubric\"]\n",
    "\n",
    "    hits = tfidf_retrieve(question, text_vec, text_X, top_k=top_k)\n",
    "    rels = []\n",
    "    for i, score in hits:\n",
    "        rels.append(is_relevant_text(page_chunks[i].text, rubric))\n",
    "\n",
    "    # Estimate total relevant in the corpus (for recall)\n",
    "    total_rel = sum(is_relevant_text(ch.text, rubric) for ch in page_chunks)\n",
    "\n",
    "    return {\n",
    "        \"id\": qobj[\"query_id\"],\n",
    "        \"P@5\": precision_at_k(rels, 5),\n",
    "        \"R@10\": recall_at_k(rels, 10, total_rel),\n",
    "        \"total_relevant_chunks\": total_rel,\n",
    "    }\n",
    "\n",
    "eval_rows = [eval_retrieval_for_query(q) for q in mini_gold]\n",
    "df_eval = pd.DataFrame(eval_rows)\n",
    "df_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t_kxiX6D75GY",
   "metadata": {
    "id": "t_kxiX6D75GY"
   },
   "outputs": [],
   "source": [
    "# Define the methods you want to compare\n",
    "# Ensure you have 'get_retrieval_results' defined from the previous step\n",
    "METHODS = [\"Sparse Only\", \"Dense Only\", \"Hybrid\", \"Hybrid + Rerank\", \"Multimodal\"]\n",
    "\n",
    "# Storage for the final table\n",
    "eval_results = []\n",
    "\n",
    "print(\"Running evaluation across all methods...\")\n",
    "\n",
    "for qobj in mini_gold:\n",
    "    qid = qobj[\"query_id\"]\n",
    "    question = qobj[\"question\"]\n",
    "    rubric = qobj[\"rubric\"]\n",
    "\n",
    "    # 1. Calculate 'Ground Truth' count (Total relevant items in corpus)\n",
    "    total_relevant_chunks = sum(is_relevant_text(ch.text, rubric) for ch in page_chunks)\n",
    "\n",
    "    # Avoid division by zero if rubric is too strict\n",
    "    if total_relevant_chunks == 0:\n",
    "        total_relevant_chunks = 1\n",
    "\n",
    "    for method in METHODS:\n",
    "        # 2. Retrieve Candidates\n",
    "        if method == \"Multimodal\":\n",
    "            # For Multimodal, we combine Hybrid Text + Sparse Image retrieval\n",
    "            text_hits = get_retrieval_results(question, \"Hybrid + Rerank\", top_k=10)\n",
    "            img_hits = tfidf_retrieve(question, img_vec, img_X, top_k=5)\n",
    "\n",
    "            # Combine them for checking (Text first, then Images)\n",
    "            # We assume the user reads text first, then looks at images\n",
    "            combined_hits = text_hits + img_hits\n",
    "\n",
    "            # Check relevance for both types\n",
    "            retrieved_is_rel = []\n",
    "            for idx, _ in text_hits:\n",
    "                retrieved_is_rel.append(is_relevant_text(page_chunks[idx].text, rubric))\n",
    "            for idx, _ in img_hits:\n",
    "                # Check image caption against rubric\n",
    "                retrieved_is_rel.append(is_relevant_text(image_items[idx].caption, rubric))\n",
    "\n",
    "        else:\n",
    "            # Standard Text Methods\n",
    "            hits = get_retrieval_results(question, method, top_k=10)\n",
    "            retrieved_is_rel = [is_relevant_text(page_chunks[idx].text, rubric) for idx, _ in hits]\n",
    "\n",
    "        # 3. Calculate Metrics\n",
    "\n",
    "        # Precision@5 (Are the top 5 relevant?)\n",
    "        p5 = precision_at_k(retrieved_is_rel, 5)\n",
    "\n",
    "        # Recall@10 (How many of the TOTAL relevant items did we find in top 10?)\n",
    "        # We look at the first 10 retrieved items\n",
    "        r10_count = sum(retrieved_is_rel[:10])\n",
    "        r10 = r10_count / total_relevant_chunks\n",
    "\n",
    "        # 4. Store Result\n",
    "        eval_results.append({\n",
    "            \"Query\": qid,\n",
    "            \"Method\": method,\n",
    "            \"Precision@5\": f\"{p5:.2f}\",\n",
    "            \"Recall@10\": f\"{r10:.2f}\",\n",
    "            \"Total_Rel_In_Corpus\": total_relevant_chunks\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame(eval_results)\n",
    "\n",
    "# Display the main table\n",
    "print(\"\\n=== Final Deliverable Table (Query x Method x Metrics) ===\")\n",
    "display(df_results)\n",
    "\n",
    "# Optional: Pivot for easier comparison of methods\n",
    "print(\"\\n=== Comparison View (Precision@5) ===\")\n",
    "display(df_results.pivot(index=\"Query\", columns=\"Method\", values=\"Precision@5\"))\n",
    "\n",
    "print(\"\\n=== Comparison View (Recall@10) ===\")\n",
    "display(df_results.pivot(index=\"Query\", columns=\"Method\", values=\"Recall@10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7629d00",
   "metadata": {
    "id": "d7629d00"
   },
   "source": [
    "# 6) Evaluation + Logging (Required)\n",
    "\n",
    "Every query must append to: `logs/query_metrics.csv`\n",
    "\n",
    "Required columns (minimum):\n",
    "- timestamp\n",
    "- query_id\n",
    "- retrieval_mode\n",
    "- top_k\n",
    "- latency_ms\n",
    "- Precision@5\n",
    "- Recall@10\n",
    "- evidence_ids_returned\n",
    "- faithfulness_pass\n",
    "- missing_evidence_behavior\n",
    "\n",
    "> If your gold set is incomplete (common for Q4/Q5), compute P/R only for labeled queries and still log latency/evidence IDs.\n",
    "\n",
    "## How we define metrics (simple)\n",
    "- `Precision@K`: (# retrieved evidence IDs in gold) / K\n",
    "- `Recall@K`: (# retrieved evidence IDs in gold) / (size of gold set)\n",
    "\n",
    "**Faithfulness (Yes/No):**\n",
    "- Yes if the answer **only** uses retrieved evidence and includes citations.\n",
    "- For this template, we implement a simple heuristic. Replace with your rubric/judge if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "850487f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "850487f0",
    "outputId": "3cff76c7-b70f-4c37-caae-e5e3936ffffa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "repr_error": "0",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-be319032-2ac6-45b3-b92a-048a3983ce4b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query_id</th>\n",
       "      <th>retrieval_mode</th>\n",
       "      <th>top_k</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>evidence_ids_returned</th>\n",
       "      <th>gold_evidence_ids</th>\n",
       "      <th>faithfulness_pass</th>\n",
       "      <th>missing_evidence_behavior</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2026-02-11T03:12:02.934843+00:00</td>\n",
       "      <td>Q5</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>10</td>\n",
       "      <td>3939.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"doc1_TimeSeries.pdf::p2\", \"figure8.png\", \"do...</td>\n",
       "      <td>[\"N/A\"]</td>\n",
       "      <td>No</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2026-02-11T03:12:04.261518+00:00</td>\n",
       "      <td>Q6</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>10</td>\n",
       "      <td>1326.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[\"doc2_ReMindRAG.pdf::p2\", \"figure3.png\", \"doc...</td>\n",
       "      <td>[\"img::figure3.png\"]</td>\n",
       "      <td>No</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2026-02-11T03:13:05.430102+00:00</td>\n",
       "      <td>Q1</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>10</td>\n",
       "      <td>7372.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[\"doc1_TimeSeries.pdf::p2\", \"figure8.png\", \"fi...</td>\n",
       "      <td>[\"doc1_TimerSeries.pdf\", \"figure8.png\", \"figur...</td>\n",
       "      <td>No</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2026-02-11T03:13:09.787266+00:00</td>\n",
       "      <td>Q2</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>10</td>\n",
       "      <td>4356.77</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[\"doc2_ReMindRAG.pdf::p8\", \"figure3.png\", \"doc...</td>\n",
       "      <td>[\"doc2_ReMindRAG.pdf\", \"figure3.png\", \"figure7...</td>\n",
       "      <td>No</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2026-02-11T03:13:11.368640+00:00</td>\n",
       "      <td>Q3</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>10</td>\n",
       "      <td>1581.03</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[\"doc3_CPP.pdf::p3\", \"figure2.png\", \"doc3_CPP....</td>\n",
       "      <td>[\"doc3_CPP.pdf\", \"figure2.png\"]</td>\n",
       "      <td>No</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2026-02-11T03:13:14.041034+00:00</td>\n",
       "      <td>Q4</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>10</td>\n",
       "      <td>2672.04</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[\"doc2_ReMindRAG.pdf::p7\", \"figure6.png\", \"doc...</td>\n",
       "      <td>[\"doc2_ReMindRAG.pdf\", \"figure6.png\"]</td>\n",
       "      <td>No</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2026-02-11T03:13:18.045480+00:00</td>\n",
       "      <td>Q5</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>10</td>\n",
       "      <td>4004.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"doc1_TimeSeries.pdf::p2\", \"figure8.png\", \"do...</td>\n",
       "      <td>[\"N/A\"]</td>\n",
       "      <td>No</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2026-02-11T03:13:20.455150+00:00</td>\n",
       "      <td>Q6</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>10</td>\n",
       "      <td>2409.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[\"doc2_ReMindRAG.pdf::p2\", \"figure3.png\", \"doc...</td>\n",
       "      <td>[\"img::figure3.png\"]</td>\n",
       "      <td>No</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be319032-2ac6-45b3-b92a-048a3983ce4b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-be319032-2ac6-45b3-b92a-048a3983ce4b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-be319032-2ac6-45b3-b92a-048a3983ce4b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                           timestamp query_id retrieval_mode  top_k  \\\n",
       "10  2026-02-11T03:12:02.934843+00:00       Q5         hybrid     10   \n",
       "11  2026-02-11T03:12:04.261518+00:00       Q6         hybrid     10   \n",
       "12  2026-02-11T03:13:05.430102+00:00       Q1         hybrid     10   \n",
       "13  2026-02-11T03:13:09.787266+00:00       Q2         hybrid     10   \n",
       "14  2026-02-11T03:13:11.368640+00:00       Q3         hybrid     10   \n",
       "15  2026-02-11T03:13:14.041034+00:00       Q4         hybrid     10   \n",
       "16  2026-02-11T03:13:18.045480+00:00       Q5         hybrid     10   \n",
       "17  2026-02-11T03:13:20.455150+00:00       Q6         hybrid     10   \n",
       "\n",
       "    latency_ms  Precision@5  Recall@10  \\\n",
       "10     3939.95          NaN        NaN   \n",
       "11     1326.36          0.0   0.000000   \n",
       "12     7372.05          0.4   0.666667   \n",
       "13     4356.77          0.2   0.666667   \n",
       "14     1581.03          0.2   0.500000   \n",
       "15     2672.04          0.2   0.500000   \n",
       "16     4004.12          NaN        NaN   \n",
       "17     2409.34          0.0   0.000000   \n",
       "\n",
       "                                evidence_ids_returned  \\\n",
       "10  [\"doc1_TimeSeries.pdf::p2\", \"figure8.png\", \"do...   \n",
       "11  [\"doc2_ReMindRAG.pdf::p2\", \"figure3.png\", \"doc...   \n",
       "12  [\"doc1_TimeSeries.pdf::p2\", \"figure8.png\", \"fi...   \n",
       "13  [\"doc2_ReMindRAG.pdf::p8\", \"figure3.png\", \"doc...   \n",
       "14  [\"doc3_CPP.pdf::p3\", \"figure2.png\", \"doc3_CPP....   \n",
       "15  [\"doc2_ReMindRAG.pdf::p7\", \"figure6.png\", \"doc...   \n",
       "16  [\"doc1_TimeSeries.pdf::p2\", \"figure8.png\", \"do...   \n",
       "17  [\"doc2_ReMindRAG.pdf::p2\", \"figure3.png\", \"doc...   \n",
       "\n",
       "                                    gold_evidence_ids faithfulness_pass  \\\n",
       "10                                            [\"N/A\"]                No   \n",
       "11                               [\"img::figure3.png\"]                No   \n",
       "12  [\"doc1_TimerSeries.pdf\", \"figure8.png\", \"figur...                No   \n",
       "13  [\"doc2_ReMindRAG.pdf\", \"figure3.png\", \"figure7...                No   \n",
       "14                    [\"doc3_CPP.pdf\", \"figure2.png\"]                No   \n",
       "15              [\"doc2_ReMindRAG.pdf\", \"figure6.png\"]                No   \n",
       "16                                            [\"N/A\"]                No   \n",
       "17                               [\"img::figure3.png\"]                No   \n",
       "\n",
       "   missing_evidence_behavior  \n",
       "10                      Pass  \n",
       "11                      Pass  \n",
       "12                      Pass  \n",
       "13                      Pass  \n",
       "14                      Pass  \n",
       "15                      Pass  \n",
       "16                      Pass  \n",
       "17                      Pass  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "def _canon_evidence_id(x: str) -> str:\n",
    "    x = str(x).strip()\n",
    "    # keep img:: prefix intact\n",
    "    if x.startswith('img::'):\n",
    "        return x\n",
    "    # normalize file ids: allow with/without extension\n",
    "    if x.endswith('.txt'):\n",
    "        return x[:-4]\n",
    "    return x\n",
    "\n",
    "def _normalize_retrieved_ids(retrieved):\n",
    "    \"\"\"Normalize retrieved outputs into a list of evidence IDs.\n",
    "    Returns canonical IDs (doc_id without .txt, or img::filename).\n",
    "\n",
    "    Supports: list[dict], list[(idx,score)], list[str].\n",
    "    \"\"\"\n",
    "    if retrieved is None:\n",
    "        return []\n",
    "    if len(retrieved) == 0:\n",
    "        return []\n",
    "    # list[str]\n",
    "    if isinstance(retrieved[0], str):\n",
    "        return [_canon_evidence_id(r) for r in retrieved]\n",
    "    # list[dict]\n",
    "    if isinstance(retrieved[0], dict):\n",
    "        out=[]\n",
    "        for r in retrieved:\n",
    "            if 'evidence_id' in r and r['evidence_id']:\n",
    "                out.append(_canon_evidence_id(r['evidence_id']))\n",
    "            elif 'doc_id' in r and r['doc_id']:\n",
    "                out.append(_canon_evidence_id(r['doc_id']))\n",
    "            elif 'source' in r and r['source']:\n",
    "                out.append(_canon_evidence_id(os.path.basename(str(r['source']))))\n",
    "            elif 'id' in r and r['id']: # Added to handle 'id' from fused evidence\n",
    "                out.append(_canon_evidence_id(r['id']))\n",
    "        return out\n",
    "    # list[(idx, score)]\n",
    "    if isinstance(retrieved[0], (tuple, list)) and len(retrieved[0]) >= 1:\n",
    "        out=[]\n",
    "        for item in retrieved:\n",
    "            idx = int(item[0])\n",
    "            if 'items' in globals() and 0 <= idx < len(items):\n",
    "                out.append(_canon_evidence_id(items[idx].get('evidence_id')))\n",
    "            elif 'documents' in globals() and 0 <= idx < len(documents):\n",
    "                out.append(_canon_evidence_id(documents[idx].get('doc_id') or os.path.basename(documents[idx].get('source',''))))\n",
    "        return out\n",
    "    return []\n",
    "\n",
    "def _normalize_gold_ids(gold_ids):\n",
    "    if not gold_ids or gold_ids == ['N/A']:\n",
    "        return None\n",
    "    return [_canon_evidence_id(g) for g in gold_ids]\n",
    "\n",
    "def precision_at_k(retrieved, gold_ids, k):\n",
    "    gold = _normalize_gold_ids(gold_ids)\n",
    "    if gold is None:\n",
    "        return None\n",
    "    retrieved_ids = _normalize_retrieved_ids(retrieved)[:k]\n",
    "    if k == 0:\n",
    "        return None\n",
    "    return len(set(retrieved_ids) & set(gold)) / float(k)\n",
    "\n",
    "def recall_at_k(retrieved, gold_ids, k):\n",
    "    gold = _normalize_gold_ids(gold_ids)\n",
    "    if gold is None:\n",
    "        return None\n",
    "    retrieved_ids = _normalize_retrieved_ids(retrieved)[:k]\n",
    "    denom = float(len(set(gold)))\n",
    "    return (len(set(retrieved_ids) & set(gold)) / denom) if denom > 0 else None\n",
    "\n",
    "\n",
    "def faithfulness_heuristic(answer: str, evidence: list):\n",
    "    # Simple heuristic: answer includes at least one citation tag from evidence OR is missing-evidence msg\n",
    "    if answer.strip() == MISSING_EVIDENCE_MSG:\n",
    "        return True\n",
    "    # Ensure evidence has 'citation_tag' for this check\n",
    "    tags = [e.get(\"citation_tag\", f\"[{e['id']}]\" if 'id' in e else \"\") for e in evidence[:5]]\n",
    "    return any(tag in answer for tag in tags)\n",
    "\n",
    "def missing_evidence_behavior(answer: str, evidence: list):\n",
    "    # Pass if either: evidence present and answer not missing-evidence; or evidence absent and answer is missing-evidence msg\n",
    "    # Use 'fused_score' if available from fused evidence, else 'score'\n",
    "    has_ev = bool(evidence) and max(e.get(\"fused_score\", e.get(\"score\", 0.0)) for e in evidence) >= 0.05\n",
    "    if not has_ev:\n",
    "        return \"Pass\" if answer.strip() == MISSING_EVIDENCE_MSG else \"Fail\"\n",
    "    else:\n",
    "        return \"Pass\" if answer.strip() != MISSING_EVIDENCE_MSG else \"Fail\"\n",
    "\n",
    "def ensure_logfile(path: str, header: list):\n",
    "    p = Path(path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not p.exists():\n",
    "        with open(p, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "\n",
    "LOG_HEADER = [\n",
    "    \"timestamp\", \"query_id\", \"retrieval_mode\", \"top_k\", \"latency_ms\",\n",
    "    \"Precision@5\", \"Recall@10\",\n",
    "    \"evidence_ids_returned\", \"gold_evidence_ids\",\n",
    "    \"faithfulness_pass\", \"missing_evidence_behavior\"\n",
    "]\n",
    "ensure_logfile(cfg.log_file, LOG_HEADER)\n",
    "\n",
    "def run_query_and_log(query_item, retrieval_mode = 'hybrid', top_k=10):\n",
    "    question = query_item[\"question\"]\n",
    "    gold_ids = query_item.get(\"gold_evidence_ids\", [])\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Build context using the dedicated function. This also performs retrieval and fusion.\n",
    "    ctx = build_context(\n",
    "        question,\n",
    "        top_k_text=TOP_K_TEXT, # Assuming TOP_K_TEXT is defined globally\n",
    "        top_k_images=TOP_K_IMAGES, # Assuming TOP_K_IMAGES is defined globally\n",
    "        top_k_evidence=TOP_K_EVIDENCE, # Assuming TOP_K_EVIDENCE is defined globally\n",
    "        alpha=ALPHA # Assuming ALPHA is defined globally\n",
    "    )\n",
    "\n",
    "    # Generate answer using the LLM with the context from ctx\n",
    "    answer = generate_llm_answer(question, ctx[\"context\"])\n",
    "    latency_ms = (time.time() - t0) * 1000.0\n",
    "\n",
    "    # Use the fused evidence from ctx for metrics and heuristic checks\n",
    "    retrieved_evidence_for_metrics = ctx[\"evidence\"]\n",
    "    retrieved_ids = _normalize_retrieved_ids(retrieved_evidence_for_metrics)\n",
    "\n",
    "    p5 = precision_at_k(retrieved_ids, gold_ids, cfg.eval_p_at) if gold_ids else np.nan\n",
    "    r10 = recall_at_k(retrieved_ids, gold_ids, cfg.eval_r_at) if gold_ids else np.nan\n",
    "\n",
    "    # Create a compatible evidence list for faithfulness and missing evidence behavior checks\n",
    "    # These functions expect 'citation_tag' and 'score' which might not be directly in fused evidence.\n",
    "    compat_evidence = []\n",
    "    for ev_item in retrieved_evidence_for_metrics:\n",
    "        compat_evidence.append({\n",
    "            \"citation_tag\": f\"[{ev_item['id']}]\", # Format citation tag from 'id'\n",
    "            \"score\": ev_item[\"fused_score\"] # Use fused_score as the score\n",
    "        })\n",
    "\n",
    "    faithful = faithfulness_heuristic(answer, compat_evidence)\n",
    "    meb = missing_evidence_behavior(answer, compat_evidence)\n",
    "\n",
    "    row = [\n",
    "        datetime.now(timezone.utc).isoformat(),\n",
    "        query_item[\"query_id\"],\n",
    "        retrieval_mode,\n",
    "        top_k,\n",
    "        round(latency_ms, 2),\n",
    "        p5,\n",
    "        r10,\n",
    "        json.dumps(retrieved_ids),\n",
    "        json.dumps(gold_ids),\n",
    "        \"Yes\" if faithful else \"No\",\n",
    "        meb\n",
    "    ]\n",
    "    with open(cfg.log_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)\n",
    "\n",
    "    return {\"answer\": answer, \"evidence\": retrieved_evidence_for_metrics, \"p5\": p5, \"r10\": r10, \"latency_ms\": latency_ms, \"faithful\": faithful, \"meb\": meb}\n",
    "\n",
    "# Run all five queries once (demo)\n",
    "results = []\n",
    "for qi in mini_gold:\n",
    "    results.append(run_query_and_log(qi, retrieval_mode = 'hybrid', top_k=cfg.top_k_default))\n",
    "\n",
    "pd.read_csv(cfg.log_file).tail(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ungUxBVhbK44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "id": "ungUxBVhbK44",
    "outputId": "58c41127-999e-418b-b059-5c1cf1dcdd2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 511.64ms\n",
      "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2528.26ms\n",
      "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 229.31ms\n",
      "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 178.01ms\n",
      "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 178.47ms\n",
      "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 152.78ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_answers\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"query_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Q1\",\n          \"Q2\",\n          \"Q6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"What is the Selective Representation Space (SRS) module in SRSNet and how does it differ from conventional adjacent patching?\",\n          \"How does ReMindRAG's memory replay mechanism improve retrieval for similar or repeated queries?\",\n          \"What are the key stages shown in the ReMindRAG overall workflow diagram?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"LLM Generation Error: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 58.046743803s. (Check your API Key)\",\n          \"LLM Generation Error: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 57.888489932s. (Check your API Key)\",\n          \"LLM Generation Error: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 54.729377016s. (Check your API Key)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evidence_ids_returned(top10)\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_evidence_ids\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_answers"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-a06d30aa-b165-484a-aa42-b00b3a7a475c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evidence_ids_returned(top10)</th>\n",
       "      <th>gold_evidence_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>What is the Selective Representation Space (SR...</td>\n",
       "      <td>LLM Generation Error: 429 POST https://generat...</td>\n",
       "      <td>[doc1_TimeSeries.pdf::p2, figure8.png, figure9...</td>\n",
       "      <td>[doc1_TimerSeries.pdf, figure8.png, figure9.png]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>How does ReMindRAG's memory replay mechanism i...</td>\n",
       "      <td>LLM Generation Error: 429 POST https://generat...</td>\n",
       "      <td>[doc2_ReMindRAG.pdf::p8, figure3.png, doc2_ReM...</td>\n",
       "      <td>[doc2_ReMindRAG.pdf, figure3.png, figure7.png]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>What real-world applications of the Consensus ...</td>\n",
       "      <td>LLM Generation Error: 429 POST https://generat...</td>\n",
       "      <td>[doc3_CPP.pdf::p3, figure2.png, doc3_CPP.pdf::...</td>\n",
       "      <td>[doc3_CPP.pdf, figure2.png]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4</td>\n",
       "      <td>According to Table 1 in the ReMindRAG paper, w...</td>\n",
       "      <td>LLM Generation Error: 429 POST https://generat...</td>\n",
       "      <td>[doc2_ReMindRAG.pdf::p7, figure6.png, doc2_ReM...</td>\n",
       "      <td>[doc2_ReMindRAG.pdf, figure6.png]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5</td>\n",
       "      <td>What reinforcement learning reward function do...</td>\n",
       "      <td>LLM Generation Error: 429 POST https://generat...</td>\n",
       "      <td>[doc1_TimeSeries.pdf::p2, figure8.png, doc1_Ti...</td>\n",
       "      <td>[N/A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q6</td>\n",
       "      <td>What are the key stages shown in the ReMindRAG...</td>\n",
       "      <td>LLM Generation Error: 429 POST https://generat...</td>\n",
       "      <td>[doc2_ReMindRAG.pdf::p2, figure3.png, doc2_ReM...</td>\n",
       "      <td>[img::figure3.png]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a06d30aa-b165-484a-aa42-b00b3a7a475c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a06d30aa-b165-484a-aa42-b00b3a7a475c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a06d30aa-b165-484a-aa42-b00b3a7a475c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_a60741ac-402b-4925-a4ff-1a7abca61812\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_answers')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_a60741ac-402b-4925-a4ff-1a7abca61812 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_answers');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "  query_id                                           question  \\\n",
       "0       Q1  What is the Selective Representation Space (SR...   \n",
       "1       Q2  How does ReMindRAG's memory replay mechanism i...   \n",
       "2       Q3  What real-world applications of the Consensus ...   \n",
       "3       Q4  According to Table 1 in the ReMindRAG paper, w...   \n",
       "4       Q5  What reinforcement learning reward function do...   \n",
       "5       Q6  What are the key stages shown in the ReMindRAG...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  LLM Generation Error: 429 POST https://generat...   \n",
       "1  LLM Generation Error: 429 POST https://generat...   \n",
       "2  LLM Generation Error: 429 POST https://generat...   \n",
       "3  LLM Generation Error: 429 POST https://generat...   \n",
       "4  LLM Generation Error: 429 POST https://generat...   \n",
       "5  LLM Generation Error: 429 POST https://generat...   \n",
       "\n",
       "                        evidence_ids_returned(top10)  \\\n",
       "0  [doc1_TimeSeries.pdf::p2, figure8.png, figure9...   \n",
       "1  [doc2_ReMindRAG.pdf::p8, figure3.png, doc2_ReM...   \n",
       "2  [doc3_CPP.pdf::p3, figure2.png, doc3_CPP.pdf::...   \n",
       "3  [doc2_ReMindRAG.pdf::p7, figure6.png, doc2_ReM...   \n",
       "4  [doc1_TimeSeries.pdf::p2, figure8.png, doc1_Ti...   \n",
       "5  [doc2_ReMindRAG.pdf::p2, figure3.png, doc2_ReM...   \n",
       "\n",
       "                                  gold_evidence_ids  \n",
       "0  [doc1_TimerSeries.pdf, figure8.png, figure9.png]  \n",
       "1    [doc2_ReMindRAG.pdf, figure3.png, figure7.png]  \n",
       "2                       [doc3_CPP.pdf, figure2.png]  \n",
       "3                 [doc2_ReMindRAG.pdf, figure6.png]  \n",
       "4                                             [N/A]  \n",
       "5                                [img::figure3.png]  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task: Run retrieval + answer generation for all mini-gold queries\n",
    "# This cell is self-contained: if retrieval/indexing cells were skipped, it will bootstrap a TF-IDF retriever.\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Build a local evidence list if not already present\n",
    "if 'items' in globals():\n",
    "    _evidence = items\n",
    "elif 'documents' in globals():\n",
    "    _evidence = []\n",
    "    for d in documents:\n",
    "        _evidence.append({\n",
    "            'evidence_id': d.get('doc_id') or os.path.basename(d.get('source','')),\n",
    "            'modality': 'text',\n",
    "            'source': d.get('source'),\n",
    "            'text': d.get('text','')\n",
    "        })\n",
    "else:\n",
    "    raise NameError('Neither items nor documents are defined. Run the ZIP extraction + document loading cells first.')\n",
    "\n",
    "assert len(_evidence) > 0, 'Evidence store is empty.'\n",
    "\n",
    "# Canonicalize evidence ids for consistent evaluation\n",
    "def _canon_evidence_id(x: str) -> str:\n",
    "    x = str(x).strip()\n",
    "    if x.startswith('img::'):\n",
    "        return x\n",
    "    return x[:-4] if x.endswith('.txt') else x\n",
    "\n",
    "# Bootstrap TF-IDF retriever if no retriever exists\n",
    "if 'retrieve_hybrid' not in globals() and 'retrieve_tfidf' not in globals() and 'retrieve' not in globals():\n",
    "    _texts = [it.get('text','') for it in _evidence]\n",
    "    _tfidf = TfidfVectorizer(stop_words=None, token_pattern=r'(?u)\\b\\w+\\b')\n",
    "    _tfidf_mat = _tfidf.fit_transform(_texts)\n",
    "\n",
    "    def retrieve_tfidf(query, top_k=10):\n",
    "        qv = _tfidf.transform([query])\n",
    "        sims = cosine_similarity(qv, _tfidf_mat).ravel()\n",
    "        idx = np.argsort(sims)[::-1][:top_k]\n",
    "        return [(int(i), float(sims[i])) for i in idx]\n",
    "\n",
    "# Define retrieve() wrapper if missing\n",
    "if 'retrieve' not in globals():\n",
    "    def retrieve(question, retrieval_mode='hybrid', top_k=10, alpha=0.6):\n",
    "        # Prefer hybrid if available; otherwise TF-IDF\n",
    "        if retrieval_mode == 'hybrid' and 'retrieve_hybrid' in globals():\n",
    "            hits = retrieve_hybrid(question, top_k=top_k, alpha=alpha)\n",
    "            return hits, {'mode':'hybrid'}\n",
    "        if 'retrieve_tfidf' in globals():\n",
    "            hits = retrieve_tfidf(question, top_k=top_k)\n",
    "            return hits, {'mode':'tfidf'}\n",
    "        raise NameError('No retriever available. Execute the retrieval/indexing section.')\n",
    "\n",
    "# Ensure build_context exists\n",
    "if 'build_context' not in globals():\n",
    "    def build_context(hit_ids, max_chars=1400):\n",
    "        parts=[]\n",
    "        for i in hit_ids:\n",
    "            parts.append(f\"[{_evidence[i].get('evidence_id')}] {_evidence[i].get('text','')}\")\n",
    "        ctx='\\n'.join(parts)\n",
    "        return ctx[:max_chars]\n",
    "\n",
    "# Ensure extractive_answer exists\n",
    "if 'extractive_answer' not in globals():\n",
    "    import re\n",
    "    def extractive_answer(query, context):\n",
    "        q=set(re.findall(r'[A-Za-z]+', query.lower()))\n",
    "        sents=re.split(r'(?<=[.!?])\\s+', (context or '').strip())\n",
    "        scored=[]\n",
    "        for s in sents:\n",
    "            w=set(re.findall(r'[A-Za-z]+', s.lower()))\n",
    "            scored.append((len(q & w), s.strip()))\n",
    "        scored.sort(key=lambda x:x[0], reverse=True)\n",
    "        best=[s for sc,s in scored[:3] if sc>0]\n",
    "        return ' '.join(best) if best else 'Not enough information in the context.'\n",
    "\n",
    "rows=[]\n",
    "for ex in mini_gold:\n",
    "    qid = ex.get('query_id')\n",
    "    question = ex.get('question')\n",
    "    gold = ex.get('gold_evidence_ids')\n",
    "\n",
    "    if 'run_query_and_log' in globals():\n",
    "        # Call run_query_and_log with the full query item dictionary 'ex'\n",
    "        out = run_query_and_log(ex, retrieval_mode='hybrid', top_k=10)\n",
    "        answer = out.get('answer')\n",
    "        # The 'evidence' key from run_query_and_log output contains a list of dicts with 'chunk_id'\n",
    "        evidence = [e['id'] for e in out.get('evidence', [])] # Changed 'chunk_id' to 'id'\n",
    "    else:\n",
    "        hits, debug = retrieve(question, retrieval_mode='hybrid', top_k=10)\n",
    "        hit_ids = [int(i) for i,_ in hits]\n",
    "        context = build_context(hit_ids[:10])\n",
    "        answer = extractive_answer(question, context)\n",
    "        evidence = [_canon_evidence_id(_evidence[i].get('evidence_id')) for i in hit_ids[:10]]\n",
    "\n",
    "    rows.append({\n",
    "        'query_id': qid,\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'evidence_ids_returned(top10)': evidence,\n",
    "        'gold_evidence_ids': gold,\n",
    "    })\n",
    "\n",
    "df_answers = pd.DataFrame(rows)\n",
    "df_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e71b22",
   "metadata": {
    "id": "46e71b22"
   },
   "source": [
    "# 7) Streamlit App Skeleton (Required)\n",
    "\n",
    "You will create a Streamlit app file in your repo, e.g.:\n",
    "\n",
    "- `app/main.py`\n",
    "\n",
    "This notebook can generate a starter `app/main.py` for your team.\n",
    "\n",
    "### Required UI components\n",
    "- Query input box\n",
    "- Retrieval controls (mode, top_k, multimodal toggle if applicable)\n",
    "- Answer panel\n",
    "- Evidence panel (with citations)\n",
    "- Metrics panel (latency, P@5, R@10 if available)\n",
    "- Logging happens automatically on each query\n",
    "\n",
    "> This skeleton calls functions in your Python modules. Prefer moving retrieval logic into `/rag/` and importing it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bb966c49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb966c49",
    "outputId": "e81da59b-8bc4-4444-91cd-a7a01356e611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote starter Streamlit app to: app/main.py\n"
     ]
    }
   ],
   "source": [
    "# Generate a starter Streamlit app file (edit paths as needed).\n",
    "# In your repo: create /app/main.py and move shared logic into /rag/\n",
    "\n",
    "streamlit_code = r'''\n",
    "import json, time\n",
    "from pathlib import Path\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# --- Import your team pipeline here ---\n",
    "# from rag.pipeline import retrieve, generate_answer, run_query_and_log\n",
    "\n",
    "MISSING_EVIDENCE_MSG = \"Not enough evidence in the retrieved context.\"\n",
    "\n",
    "st.set_page_config(page_title=\"CS5542 Lab 4 — Project RAG App\", layout=\"wide\")\n",
    "st.title(\"CS 5542 Lab 4 — Project RAG Application\")\n",
    "st.caption(\"Project-aligned Streamlit UI + automatic logging + failure monitoring\")\n",
    "\n",
    "# Sidebar controls\n",
    "st.sidebar.header(\"Retrieval Settings\")\n",
    "retrieval_mode = st.sidebar.selectbox(\"retrieval_mode\", [\"tfidf\", \"dense\", \"sparse\", \"hybrid\", \"hybrid_rerank\"])\n",
    "top_k = st.sidebar.slider(\"top_k\", min_value=1, max_value=30, value=10, step=1)\n",
    "use_multimodal = st.sidebar.checkbox(\"use_multimodal\", value=True)\n",
    "\n",
    "st.sidebar.header(\"Logging\")\n",
    "log_path = st.sidebar.text_input(\"log file\", value=\"logs/query_metrics.csv\")\n",
    "\n",
    "# --- Mini gold set (replace with your team's Q1–Q5) ---\n",
    "# Tip: keep the same structure as in your Lab 4 notebook so IDs match logs.\n",
    "MINI_GOLD = {\n",
    "    \"Q1\": {\"question\": What is the Selective Representation Space (SRS) module in SRSNet and how does it differ from conventional adjacent patching?\", \"gold_evidence_ids\": ['doc1_TimerSeries.pdf']\n",
    "    },\n",
    "    \"Q2\": {\"question\": \"How does ReMindRAG\\'s memory replay mechanism improve retrieval for similar or repeated queries?\", \"gold_evidence_ids\": ['doc2_ReMindRAG.pdf']},\n",
    "    \"Q3\": {\"question\": \"What real-world applications of the Consensus Planning Problem are described, and what agent interfaces does each application use?\", \"gold_evidence_ids\": ['doc3_CPP.pdf']},\n",
    "    \"Q4\": {\"question\": \"According to Table 1 in the ReMindRAG paper, what is the Multi-Hop QA accuracy of ReMindRAG with the Deepseek-V3 backbone compared to HippoRAG2?\", \"gold_evidence_ids\": ['img::figure6.png']},\n",
    "    \"Q5\": {\"question\": \"What reinforcement learning reward function does SRSNet use to train the Selective Patching scorer?\", \"gold_evidence_ids\": ['N/A']},\n",
    "}\n",
    "\n",
    "st.sidebar.header(\"Evaluation\")\n",
    "query_id = st.sidebar.selectbox(\"query_id (for logging)\", list(MINI_GOLD.keys()))\n",
    "use_gold_question = st.sidebar.checkbox(\"Use the gold-set question text\", value=True)\n",
    "\n",
    "# Main query\n",
    "default_q = MINI_GOLD[query_id][\"question\"] if use_gold_question else \"\"\n",
    "question = st.text_area(\"Enter your question\", value=default_q, height=120)\n",
    "run_btn = st.button(\"Run Query\")\n",
    "\n",
    "colA, colB = st.columns([2, 1])\n",
    "\n",
    "def ensure_logfile(path: str):\n",
    "    p = Path(path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not p.exists():\n",
    "        df = pd.DataFrame(columns=[\n",
    "            \"timestamp\",\"query_id\",\"retrieval_mode\",\"top_k\",\"latency_ms\",\n",
    "            \"Precision@5\",\"Recall@10\",\"evidence_ids_returned\",\"gold_evidence_ids\",\n",
    "            \"faithfulness_pass\",\"missing_evidence_behavior\"\n",
    "        ])\n",
    "        df.to_csv(p, index=False)\n",
    "\n",
    "def precision_at_k(retrieved_ids, gold_ids, k=5):\n",
    "    if not gold_ids:\n",
    "        return None\n",
    "    topk = retrieved_ids[:k]\n",
    "    hits = sum(1 for x in topk if x in set(gold_ids))\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k(retrieved_ids, gold_ids, k=10):\n",
    "    if not gold_ids:\n",
    "        return None\n",
    "    topk = retrieved_ids[:k]\n",
    "    hits = sum(1 for x in topk if x in set(gold_ids))\n",
    "    return hits / max(1, len(gold_ids))\n",
    "\n",
    "# ---- Placeholder demo logic (replace with imports from your /rag module) ----\n",
    "def retrieve_demo(q: str, top_k: int):\n",
    "    return [{\"chunk_id\":\"demo_doc\",\"citation_tag\":\"[demo_doc]\",\"score\":0.9,\"source\":\"data/docs/demo_doc.txt\",\"text\":\"demo evidence...\"}]\n",
    "\n",
    "def answer_demo(q: str, evidence: list):\n",
    "    if not evidence:\n",
    "        return MISSING_EVIDENCE_MSG\n",
    "    return f\"Grounded answer using {evidence[0]['citation_tag']} {evidence[0]['citation_tag']}\"\n",
    "\n",
    "def log_row(path: str, row: dict):\n",
    "    ensure_logfile(path)\n",
    "    df = pd.read_csv(path)\n",
    "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "    df.to_csv(path, index=False)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "if run_btn and question.strip():\n",
    "    t0 = time.time()\n",
    "    evidence = retrieve_demo(question, top_k=top_k)\n",
    "    answer = answer_demo(question, evidence)\n",
    "    latency_ms = round((time.time() - t0)*1000, 2)\n",
    "\n",
    "    retrieved_ids = [e[\"chunk_id\"] for e in evidence]\n",
    "    gold_ids = MINI_GOLD[query_id].get(\"gold_evidence_ids\", [])\n",
    "\n",
    "    p5 = precision_at_k(retrieved_ids, gold_ids, k=5)\n",
    "    r10 = recall_at_k(retrieved_ids, gold_ids, k=10)\n",
    "\n",
    "    with colA:\n",
    "        st.subheader(\"Answer\")\n",
    "        st.write(answer)\n",
    "\n",
    "        st.subheader(\"Evidence (Top-K)\")\n",
    "        st.json(evidence)\n",
    "\n",
    "    with colB:\n",
    "        st.subheader(\"Metrics\")\n",
    "        st.write({\"latency_ms\": latency_ms, \"Precision@5\": p5, \"Recall@10\": r10})\n",
    "\n",
    "    # Log the query using the selected Q1–Q5 ID (not ad-hoc)\n",
    "    row = {\n",
    "        \"timestamp\": pd.Timestamp.utcnow().isoformat(),\n",
    "        \"query_id\": query_id,\n",
    "        \"retrieval_mode\": retrieval_mode,\n",
    "        \"top_k\": top_k,\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"Precision@5\": p5,\n",
    "        \"Recall@10\": r10,\n",
    "        \"evidence_ids_returned\": json.dumps(retrieved_ids),\n",
    "        \"gold_evidence_ids\": json.dumps(gold_ids),\n",
    "        \"faithfulness_pass\": \"Yes\" if answer != MISSING_EVIDENCE_MSG else \"Yes\",\n",
    "        \"missing_evidence_behavior\": \"Pass\"  # update with your rule if needed\n",
    "    }\n",
    "    log_row(log_path, row)\n",
    "    st.success(f\"Logged {query_id} to CSV.\")\n",
    "'''\n",
    "app_dir = Path(\"app\")\n",
    "app_dir.mkdir(parents=True, exist_ok=True)\n",
    "(app_dir / \"main.py\").write_text(streamlit_code, encoding=\"utf-8\")\n",
    "print(\"Wrote starter Streamlit app to:\", app_dir / \"main.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c21ec8",
   "metadata": {
    "id": "05c21ec8"
   },
   "source": [
    "# 8) Optional Extension — FastAPI Backend (Recommended for larger teams)\n",
    "\n",
    "If your team selects the **FastAPI extension**, create:\n",
    "- `api/server.py` with `POST /query`\n",
    "- Streamlit UI calls the API using `requests.post(...)`\n",
    "\n",
    "This separation mirrors real production systems:\n",
    "UI (Streamlit) → API (FastAPI) → Retrieval + LLM services\n",
    "\n",
    "Below is a minimal FastAPI starter you can generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32168ff2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32168ff2",
    "outputId": "1b79abfa-721a-4d13-9a4e-292092e67a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote starter FastAPI server to: api/server.py\n",
      "\n",
      "Run locally (terminal):\n",
      "  uvicorn api.server:app --reload --port 8000\n"
     ]
    }
   ],
   "source": [
    "fastapi_code = r'''\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "app = FastAPI(title=\"CS5542 Lab 4 RAG Backend\")\n",
    "\n",
    "MISSING_EVIDENCE_MSG = \"Not enough evidence in the retrieved context.\"\n",
    "\n",
    "class QueryIn(BaseModel):\n",
    "    question: str\n",
    "    top_k: int = 10\n",
    "    retrieval_mode: str = \"hybrid\"\n",
    "    use_multimodal: bool = True\n",
    "\n",
    "@app.post(\"/query\")\n",
    "def query(q: QueryIn) -> Dict[str, Any]:\n",
    "    # TODO: import your real pipeline:\n",
    "    # evidence = retrieve(q.question, top_k=q.top_k, mode=q.retrieval_mode, use_multimodal=q.use_multimodal)\n",
    "    # answer = generate_answer(q.question, evidence)\n",
    "    evidence = [{\"chunk_id\":\"demo_doc\",\"citation_tag\":\"[demo_doc]\",\"score\":0.9,\"source\":\"data/docs/demo_doc.txt\",\"text\":\"demo evidence...\"}]\n",
    "    answer = f\"Grounded answer using {evidence[0]['citation_tag']} {evidence[0]['citation_tag']}\"\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"evidence\": evidence,\n",
    "        \"metrics\": {\"top_k\": q.top_k, \"retrieval_mode\": q.retrieval_mode},\n",
    "        \"failure_flag\": False\n",
    "    }\n",
    "'''\n",
    "api_dir = Path(\"api\")\n",
    "api_dir.mkdir(parents=True, exist_ok=True)\n",
    "(api_dir / \"server.py\").write_text(fastapi_code, encoding=\"utf-8\")\n",
    "print(\"Wrote starter FastAPI server to:\", api_dir / \"server.py\")\n",
    "\n",
    "print(\"\\nRun locally (terminal):\")\n",
    "print(\"  uvicorn api.server:app --reload --port 8000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D7gTOwkFwPlc",
   "metadata": {
    "id": "D7gTOwkFwPlc"
   },
   "outputs": [],
   "source": [
    "# Temporary Build\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# We embed the pipeline logic (loading, indexing, retrieval) directly into the server file\n",
    "# so it runs independently of the notebook kernel.\n",
    "fastapi_code = r'''\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "app = FastAPI(title=\"CS5542 Lab 4 RAG Backend\")\n",
    "\n",
    "# --- 1. Global State & Data Loading ---\n",
    "# We store the index and evidence globally so they load once on startup\n",
    "global_state = {\n",
    "    \"vectorizer\": None,\n",
    "    \"tfidf_matrix\": None,\n",
    "    \"evidence\": []\n",
    "}\n",
    "\n",
    "def load_data_and_index():\n",
    "    \"\"\"Loads text files from ./data/docs and builds a TF-IDF index.\"\"\"\n",
    "    print(\"Loading data from ./data/docs...\")\n",
    "    docs_dir = \"./data/docs\"\n",
    "\n",
    "    # Simple loader matching your notebook's logic\n",
    "    items = []\n",
    "    if os.path.exists(docs_dir):\n",
    "        files = glob.glob(os.path.join(docs_dir, \"*.txt\")) + glob.glob(os.path.join(docs_dir, \"*.pdf\"))\n",
    "        for p in files:\n",
    "            # For simplicity in this demo server, we read text files directly.\n",
    "            # If using PDFs, you'd include PyMuPDF logic here or assume pre-converted .txts exist.\n",
    "            try:\n",
    "                # Fallback: try reading as text (works for the .txt demo files)\n",
    "                with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    text = f.read()\n",
    "\n",
    "                items.append({\n",
    "                    \"evidence_id\": os.path.basename(p),\n",
    "                    \"source\": p,\n",
    "                    \"text\": text,\n",
    "                    \"citation_tag\": f\"[{os.path.basename(p)}]\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping file {p}: {e}\")\n",
    "\n",
    "    if not items:\n",
    "        print(\"WARNING: No documents found in ./data/docs. Server will have empty index.\")\n",
    "        return\n",
    "\n",
    "    # Build TF-IDF Index\n",
    "    texts = [it[\"text\"] for it in items]\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    global_state[\"evidence\"] = items\n",
    "    global_state[\"vectorizer\"] = vectorizer\n",
    "    global_state[\"tfidf_matrix\"] = tfidf_matrix\n",
    "    print(f\"Server ready: Indexed {len(items)} documents.\")\n",
    "\n",
    "# Load on startup\n",
    "@app.on_event(\"startup\")\n",
    "def startup_event():\n",
    "    load_data_and_index()\n",
    "\n",
    "# --- 2. Your Pipeline Functions (Ported from Notebook) ---\n",
    "\n",
    "def retrieve_tfidf(question: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "    vec = global_state[\"vectorizer\"]\n",
    "    mat = global_state[\"tfidf_matrix\"]\n",
    "\n",
    "    if vec is None or mat is None:\n",
    "        return []\n",
    "\n",
    "    q_vec = vec.transform([question])\n",
    "    sims = cosine_similarity(q_vec, mat).ravel()\n",
    "    # Get top_k indices\n",
    "    idxs = np.argsort(-sims)[:top_k]\n",
    "    return [(int(i), float(sims[i])) for i in idxs]\n",
    "\n",
    "def build_context(hit_ids: List[int], max_chars=2000) -> str:\n",
    "    parts = []\n",
    "    current_len = 0\n",
    "    for i in hit_ids:\n",
    "        item = global_state[\"evidence\"][i]\n",
    "        text = item[\"text\"]\n",
    "        tag = item[\"citation_tag\"]\n",
    "        # Simple truncation for context window\n",
    "        entry = f\"{tag} {text}\"\n",
    "        if current_len + len(entry) > max_chars:\n",
    "            break\n",
    "        parts.append(entry)\n",
    "        current_len += len(entry)\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "def extractive_answer(query: str, context: str) -> str:\n",
    "    \"\"\"Simple heuristic answer generator from your notebook.\"\"\"\n",
    "    if not context.strip():\n",
    "        return \"Not enough evidence in the retrieved context.\"\n",
    "\n",
    "    # Heuristic: Find sentences with overlapping words\n",
    "    q_words = set(re.findall(r'[A-Za-z]+', query.lower()))\n",
    "    # Split by simple punctuation\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', context)\n",
    "\n",
    "    scored = []\n",
    "    for s in sents:\n",
    "        w = set(re.findall(r'[A-Za-z]+', s.lower()))\n",
    "        score = len(q_words & w)\n",
    "        if score > 0:\n",
    "            scored.append((score, s.strip()))\n",
    "\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Return top 3 sentences or fallback\n",
    "    best = [s for sc, s in scored[:3]]\n",
    "    if best:\n",
    "        return \" \".join(best)\n",
    "    return \"Not enough evidence in the retrieved context.\"\n",
    "\n",
    "# --- 3. API Endpoint ---\n",
    "\n",
    "class QueryIn(BaseModel):\n",
    "    question: str\n",
    "    top_k: int = 5\n",
    "    retrieval_mode: str = \"hybrid\" # We use tfidf fallback in this server\n",
    "\n",
    "@app.post(\"/query\")\n",
    "def query(q: QueryIn) -> Dict[str, Any]:\n",
    "    # 1. Retrieve\n",
    "    # (Currently forcing TF-IDF pipeline for the server demo)\n",
    "    hits = retrieve_tfidf(q.question, top_k=q.top_k)\n",
    "\n",
    "    # 2. Format Evidence\n",
    "    evidence_list = []\n",
    "    hit_indices = []\n",
    "    for idx, score in hits:\n",
    "        item = global_state[\"evidence\"][idx]\n",
    "        evidence_list.append({\n",
    "            \"chunk_id\": item[\"evidence_id\"],\n",
    "            \"citation_tag\": item[\"citation_tag\"],\n",
    "            \"score\": score,\n",
    "            \"source\": item[\"source\"],\n",
    "            \"text\": item[\"text\"][:500] + \"...\" # Truncate for API response payload\n",
    "        })\n",
    "        hit_indices.append(idx)\n",
    "\n",
    "    # 3. Generate Answer\n",
    "    context = build_context(hit_indices)\n",
    "    answer = extractive_answer(q.question, context)\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"evidence\": evidence_list,\n",
    "        \"metrics\": {\n",
    "            \"top_k\": q.top_k,\n",
    "            \"retrieval_mode\": \"tfidf_server_baseline\"\n",
    "        },\n",
    "        \"failure_flag\": answer == \"Not enough evidence in the retrieved context.\"\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    # Allow running directly via python api/server.py\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "api_dir = Path(\"api\")\n",
    "api_dir.mkdir(parents=True, exist_ok=True)\n",
    "(api_dir / \"server.py\").write_text(fastapi_code, encoding=\"utf-8\")\n",
    "print(\"✅ Wrote self-contained FastAPI server to:\", api_dir / \"server.py\")\n",
    "print(\"\\nRun locally (terminal):\")\n",
    "print(\"  uvicorn api.server:app --reload --port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351032a",
   "metadata": {
    "id": "9351032a"
   },
   "source": [
    "# 9) Deployment checklist (Required)\n",
    "\n",
    "Choose **one** deployment route and publish the public link in your README:\n",
    "\n",
    "- HuggingFace Spaces (Streamlit)\n",
    "- Streamlit Cloud (GitHub-connected)\n",
    "- Render / Railway (GitHub-connected)\n",
    "\n",
    "## README must include\n",
    "1. Public deployment link  \n",
    "2. How to run locally:\n",
    "   - `pip install -r requirements.txt`\n",
    "   - `streamlit run app/main.py`\n",
    "3. A screenshot of:\n",
    "   - the UI\n",
    "   - evidence panel\n",
    "   - metrics panel\n",
    "4. Results snapshot:\n",
    "   - **5 queries × 2 retrieval modes**\n",
    "5. Failure analysis:\n",
    "   - 2 failure cases, root cause, proposed fix\n",
    "\n",
    "---\n",
    "\n",
    "# 10) Failure analysis template (Required)\n",
    "\n",
    "Document:\n",
    "1. **Retrieval failure** (wrong evidence or missed gold evidence)  \n",
    "2. **Grounding / missing-evidence failure** (safe behavior or citation enforcement)\n",
    "\n",
    "For each:\n",
    "- What happened?\n",
    "- Why did it happen (root cause)?\n",
    "- What change will you implement next?\n",
    "\n",
    "You can paste your analysis into your README under **Lab 4 Results**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c11a7f",
   "metadata": {
    "id": "67c11a7f"
   },
   "source": [
    "# 11) Team checklist (quick)\n",
    "\n",
    "Before submission, verify:\n",
    "\n",
    "- [ ] Dataset, UI, and models are **project-aligned**\n",
    "- [ ] Streamlit app runs locally and shows: answer + evidence + metrics\n",
    "- [ ] `logs/query_metrics.csv` is auto-created and appended per query\n",
    "- [ ] Mini gold set Q1–Q5 exists and P@5/R@10 computed when possible\n",
    "- [ ] Deployed link is public and listed in README\n",
    "- [ ] Two failure cases documented with fixes\n",
    "- [ ] `requirements.txt` and run instructions are correct\n",
    "- [ ] Individual survey submitted by each teammate\n",
    "\n",
    "---\n",
    "\n",
    "If you want to go beyond: add an evaluation dashboard, reranking integration, or FastAPI separation (extensions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4l9AWSmiSN26",
   "metadata": {
    "id": "4l9AWSmiSN26"
   },
   "outputs": [],
   "source": [
    "# Verification: retrieval should return non-empty results for the demo queries\n",
    "test_q = \"What is Retrieval-Augmented Generation (RAG) and what does grounding mean?\"\n",
    "\n",
    "try:\n",
    "    # Try common retrieval function names in the template\n",
    "    if 'retrieve_tfidf' in globals():\n",
    "        hits = retrieve_tfidf(test_q, top_k=5)\n",
    "    elif 'retrieve' in globals():\n",
    "        hits = retrieve(test_q, top_k=5)\n",
    "    else:\n",
    "        hits = []\n",
    "    # Normalize hits to a list\n",
    "    if hits is None:\n",
    "        hits = []\n",
    "    # Some functions return list of dicts; some return (idx,score)\n",
    "    n = len(hits) if hasattr(hits, '__len__') else 0\n",
    "    print('Demo retrieval hits:', n)\n",
    "    assert n > 0, 'Retrieval returned empty results. Check indexing cell and corpus construction.'\n",
    "except Exception as e:\n",
    "    print('⚠️ Retrieval verification could not run (template function names differ).')\n",
    "    print('Reason:', type(e).__name__, str(e)[:180])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71062ba0",
   "metadata": {
    "id": "71062ba0"
   },
   "source": [
    "\n",
    "## GitHub Deployment Example\n",
    "\n",
    "### Step 1 — Push to GitHub\n",
    "```bash\n",
    "git init\n",
    "git add .\n",
    "git commit -m \"Lab4 deployment\"\n",
    "git branch -M main\n",
    "git remote add origin https://github.com/<username>/<repo>.git\n",
    "git push -u origin main\n",
    "```\n",
    "\n",
    "### Step 2 — Deploy using Streamlit Cloud\n",
    "1. Visit https://share.streamlit.io\n",
    "2. Click **New App**\n",
    "3. Select your GitHub repository\n",
    "4. Branch: `main`\n",
    "5. App path: `app/main.py`\n",
    "6. Click **Deploy**\n",
    "\n",
    "### Step 3 — Add deployment link\n",
    "Include the deployed application URL in your README.md file.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
